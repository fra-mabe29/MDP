{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install SciExpeM-API \n!pip install SciExpeM-API --upgrade","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:12:55.756044Z","iopub.execute_input":"2022-05-22T12:12:55.756943Z","iopub.status.idle":"2022-05-22T12:13:22.056914Z","shell.execute_reply.started":"2022-05-22T12:12:55.756864Z","shell.execute_reply":"2022-05-22T12:13:22.055498Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from SciExpeM_API.SciExpeM import SciExpeM\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \ndb = SciExpeM(username='manuel.peracci', password='mdp2022_',verify=False,warning=False)\ndb.testConnection(verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:13:22.752792Z","iopub.execute_input":"2022-05-22T12:13:22.753128Z","iopub.status.idle":"2022-05-22T12:13:23.174352Z","shell.execute_reply.started":"2022-05-22T12:13:22.753099Z","shell.execute_reply":"2022-05-22T12:13:23.173380Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from scipy.interpolate import CubicSpline\nimport matplotlib.pyplot as plt\nimport random\nrandom.seed(2)\nimport pandas as pd\nimport json\n\n# ex.__dict__\n\ndef list_duplicates(seq):\n  seen = set()\n  seen_add = seen.add\n  seen_twice = set( x for x in seq if x in seen or seen_add(x) )\n  return list( seen_twice )\n\ndef normalizeArray(ar):\n    return (ar - np.mean(ar))/(np.std(ar))\n\ndef deNorm(ar,std,mean):\n    return ar * std + mean \n\ndef random_matrix(data_aug,a,b,arr): \n    listoflists = []\n    for ele in range(0,data_aug):\n        a_list = []\n        for ala in range(0,32):\n            a_list.append(random.uniform(a*np.std(arr),b*np.std(arr)))\n        listoflists.append(a_list)\n    return np.array(listoflists)\n\n# it extracts some data from the experiment type laminar burning velocity measurement\n# ex_id = it is the experiment identifier\n# num_data_aug = it is the number of desired data augmentation (notice that is always mulplied by 3)\n# chemModel_id = the model identifier which can be omitted \ndef prova(ex_id,num_data_aug,chemModel_id = 0): \n    \n    if(chemModel_id == 0):\n        ex = db.filterDatabase(model_name='Execution', experiment__id=ex_id)\n    else:\n        ex = db.filterDatabase(model_name='Execution', experiment__id=ex_id, chemModel__id=chemModel_id)\n        \n    #if it is empty, return an empty df\n    if not ex:\n        return pd.DataFrame()\n    else:\n        ex = ex[0]\n    \n    # find the right data_columns\n    exp =[ex.experiment.pairs[0][\"x\"],ex.experiment.pairs[0][\"y\"]]\n    \n    if(exp[0].units != 'unitless' or exp[1].units != 'cm/s'):\n        print(\"Warning: different units\")\n        return pd.DataFrame()\n    \n    xx = np.round(exp[0].data,7)\n    yy = np.round(exp[1].data,7)\n    dupl = list_duplicates(xx)\n\n    # In case of duplicates, we decided to leave only the first one \n    # It caused problems to the CubicSpline function\n    if dupl:\n        for i in list_duplicates(xx):\n\n            c = np.where(xx == np.round(i,7))[0]\n            c = c[1:np.size(c)]\n\n            yy = np.delete(yy,c)\n            xx = np.delete(xx,c)\n            \n        dupl = list_duplicates(xx)  \n        # if there are still duplicates, send a warning \n        if dupl:\n            print(\"Warning: duplicates found\")\n            print(\"duplicates:\",dupl)\n    \n    # compute mean and variance that there will come in handy later \n    exp_mean = np.column_stack((np.mean(xx),np.mean(yy)))[0]\n    exp_std = np.column_stack((np.std(xx),np.std(yy)))[0]\n    \n    # normalize the values on the x-axis and y-axis\n    orig = np.column_stack((normalizeArray(np.array(xx)),normalizeArray(np.array(yy))))\n    \n    # order the values for the CubicSpline function \n    orig_s = orig[np.argsort(orig[:,0])].T\n\n    # compute some random values that will be used either to traslate or dilate the original function\n    # these values are based on the standard deviation of x \n    rando = random_matrix(num_data_aug,-1,1,orig_s[1]) #np.array([random.uniform((-1)*np.std(orig_s[1]),np.std(orig_s[1])) for ele in range(0,num_data_aug)])  \n    d = random_matrix(num_data_aug,0.5,2,orig_s[1]) # np.array([random.uniform((0.5)*np.std(orig_s[1]),2*np.std(orig_s[1])) for ele in range(0,num_data_aug)])\n    rando2 = random_matrix(num_data_aug,0.1,1,orig_s[1]) # np.array([random.uniform(0.1,1*np.std(orig_s[1])) for ele in range(0,num_data_aug)])\n    \n    # in order not to use always the same points on the x-axis for the curvematching, we came up with this solution:\n    # we split the array on the x-axis into sub-arrays and on each of them we picked a random value \n    # we used all these new random values as new points on the x-axis\n    # notice that we chose this because we wanted to keep some kind of uniform distribution \n    # for the points and also the same number of points\n    newarr = np.array_split(orig_s[0], len(orig_s[0]))\n    x_n = []\n    for ele in range(0,len(newarr)):\n        if ele == (len(newarr) - 1): break\n        x_n.append(random.uniform(newarr[ele],newarr[ele+1])[0])\n    \n    # we decided out of coherence to use the cubic interpolation because it was used also in the curve matching algorithm\n    cs = CubicSpline(orig_s[0],orig_s[1]) \n    ls_f1 = []\n    ls_f2 = []\n    ls_f3 = []\n    ls_f4 = []\n    ls_f5 = []\n    df = pd.DataFrame(columns=[\"score\",\"error\",\"d0L2\",\"d1L2\",\"d0Pe\",\"d1Pe\",\"shift\"])\n     \n    \n    # data augmentation on the curve created from the original experiment \n    # notice that everything is being normalized because the experiments have different scales \n    # and then everything is denormalized in order to store correctly the data on the dataframe\n    for data_aug in range(0,num_data_aug):  \n        \n        rando_de = deNorm(rando[data_aug],exp_std[1],exp_mean[1])\n        \n        # 1 vertical traslation\n        x_de_n = np.ndarray.tolist(deNorm(np.array(x_n),exp_std[0],exp_mean[0]))\n        y_n = cs(x_n)\n        tras_y_n = cs(x_n) + rando[data_aug,1]\n        y_de_n = np.ndarray.tolist(deNorm(np.array(y_n),exp_std[1],exp_mean[1]))\n        tras_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_y_n),exp_std[1],rando_de[1]))\n\n        # 2 vertical dilatation\n        tras_2_y_n = cs(x_n) * d[data_aug,2]\n        tras_2_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_2_y_n),exp_std[1] * d[data_aug,2],exp_mean[1]))\n\n        # 3 vertical traslation + vertical dilatation\n        tras_3_y_n = (cs(x_n) + rando[data_aug,3]) * d[data_aug,3]\n        tras_3_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_3_y_n),exp_std[1] * d[data_aug,3],exp_mean[1] + exp_std[1] * rando[data_aug,3] * d[data_aug,3]))\n       \n        # 4 max as outlier -> incremented with a random number between 0 and 1 std\n        pos_max = np.argmax(y_n)\n        tras_4_y_n = y_n\n        tras_4_y_n[pos_max] = tras_4_y_n[pos_max] * (1 + rando2[data_aug,4])\n        tras_4_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_4_y_n),exp_std[1],exp_mean[1]))\n        \n        # 5 vertical traslation + max as outlier\n        tras_5_y_n = y_n + rando[data_aug,5]\n        tras_5_y_n[pos_max] = tras_5_y_n[pos_max] * (1 + rando2[data_aug,5])\n        tras_5_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_5_y_n),exp_std[1],rando_de[5]))\n        \n        # 6 vertical dilatation + max as outlier\n        tras_6_y_n = y_n * d[data_aug,6]\n        tras_6_y_n[pos_max] = tras_6_y_n[pos_max] * (1 + rando2[data_aug,6])\n        tras_6_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_6_y_n),exp_std[1] * d[data_aug,6],exp_mean[1]))\n        \n        # 7 vertical traslation + vertical dilatation + max as outlier\n        tras_7_y_n = (cs(x_n) + rando[data_aug,7]) * d[data_aug,7]\n        tras_7_y_n[pos_max] = tras_7_y_n[pos_max] * (1 + rando2[data_aug,7])\n        tras_7_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_7_y_n),exp_std[1] * d[data_aug,7],exp_mean[1] + exp_std[1] * rando[data_aug,7] * d[data_aug,7]))\n      \n        # 8 min as outlier -> decreased by a random number between 0 and 1 std\n        pos_min = np.argmin(y_n)\n        tras_8_y_n = y_n\n        tras_8_y_n[pos_min] = tras_8_y_n[pos_min] * (1 - rando2[data_aug,8])\n        tras_8_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_8_y_n),exp_std[1],exp_mean[1]))\n        \n        # 9 vertical traslation + min as outlier\n        tras_9_y_n = y_n + rando[data_aug,9]\n        tras_9_y_n[pos_min] = tras_9_y_n[pos_min] * (1 - rando2[data_aug,9])\n        tras_9_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_9_y_n),exp_std[1],rando_de[9]))\n        \n        # 10 vertical dilatation + min as outlier\n        tras_10_y_n = y_n * d[data_aug,10]\n        tras_10_y_n[pos_min] = tras_10_y_n[pos_min] * (1 - rando2[data_aug,10])\n        tras_10_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_10_y_n),exp_std[1] * d[data_aug,10],exp_mean[1]))\n        \n        # 11 vertical traslation + vertical dilatation + min as outlier\n        tras_11_y_n = (cs(x_n) + rando[data_aug,11]) * d[data_aug,11]\n        tras_11_y_n[pos_min] = tras_11_y_n[pos_min] * (1 - rando2[data_aug,11])\n        tras_11_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_11_y_n),exp_std[1] * d[data_aug,11],exp_mean[1] + exp_std[1] * rando[data_aug,11] * d[data_aug,11]))\n      \n        # 12 min and max as outliers \n        tras_12_y_n = y_n\n        tras_12_y_n[pos_min] = tras_12_y_n[pos_min] * (1 - rando2[data_aug,12])\n        tras_12_y_n[pos_max] = tras_12_y_n[pos_max] * (1 + rando2[data_aug,12])\n        tras_12_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_12_y_n),exp_std[1],exp_mean[1]))\n        \n        # 13 vertical traslation + min and max as outliers \n        tras_13_y_n = y_n + rando[data_aug,13]\n        tras_13_y_n[pos_min] = tras_13_y_n[pos_min] * (1 - rando2[data_aug,13])\n        tras_13_y_n[pos_max] = tras_13_y_n[pos_max] * (1 + rando2[data_aug,13])\n        tras_13_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_13_y_n),exp_std[1],rando_de[13]))\n        \n        # 14 vertical dilatation + min and max as outliers \n        tras_14_y_n = y_n * d[data_aug,14]\n        tras_14_y_n[pos_min] = tras_14_y_n[pos_min] * (1 - rando2[data_aug,14])\n        tras_14_y_n[pos_max] = tras_14_y_n[pos_max] * (1 + rando2[data_aug,14])\n        tras_14_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_14_y_n),exp_std[1] * d[data_aug,14],exp_mean[1]))\n        \n        # 15 vertical traslation + vertical dilatation + min and max as outliers\n        tras_15_y_n = (cs(x_n) + rando[data_aug,15]) * d[data_aug,15]\n        tras_15_y_n[pos_min] = tras_15_y_n[pos_min] * (1 - rando2[data_aug,15])\n        tras_15_y_n[pos_max] = tras_15_y_n[pos_max] * (1 + rando2[data_aug,15])\n        tras_15_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_15_y_n),exp_std[1] * d[data_aug,15],exp_mean[1] + exp_std[1] * rando[data_aug,15] * d[data_aug,15]))      \n       \n        # 16 random point -> random modification of a point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_16_y_n = y_n\n        tras_16_y_n[pos_ran] = tras_16_y_n[pos_ran] * (1 + rando[data_aug,16])\n        tras_16_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_16_y_n),exp_std[1],exp_mean[1]))\n        \n        # 17 vertical traslation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_17_y_n = y_n + rando[data_aug,17]\n        tras_17_y_n[pos_ran] = tras_17_y_n[pos_ran] * (1 + rando[data_aug,17])\n        tras_17_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_17_y_n),exp_std[1],rando_de[17]))\n        \n        # 18 vertical dilatation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_18_y_n = y_n * d[data_aug,18]\n        tras_18_y_n[pos_ran] = tras_18_y_n[pos_ran] * (1 + rando[data_aug,18])\n        tras_18_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_18_y_n),exp_std[1] * d[data_aug,18],exp_mean[1]))\n        \n        # 19 max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_19_y_n = y_n \n        tras_19_y_n[pos_ran] = tras_19_y_n[pos_ran] * (1 + rando[data_aug,19])\n        tras_19_y_n[pos_max] = tras_19_y_n[pos_max] * (1 + rando2[data_aug,19])\n        tras_19_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_19_y_n),exp_std[1],exp_mean[1]))\n        \n        # 20 min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_20_y_n = y_n \n        tras_20_y_n[pos_ran] = tras_20_y_n[pos_ran] * (1 + rando[data_aug,20])\n        tras_20_y_n[pos_min] = tras_20_y_n[pos_min] * (1 - rando2[data_aug,20])        \n        tras_20_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_20_y_n),exp_std[1],exp_mean[1]))\n        \n        # 21 max and min as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_21_y_n = y_n \n        tras_21_y_n[pos_ran] = tras_21_y_n[pos_ran] * (1 + rando[data_aug,21])\n        tras_21_y_n[pos_min] = tras_21_y_n[pos_min] * (1 - rando2[data_aug,21])  \n        tras_21_y_n[pos_max] = tras_21_y_n[pos_max] * (1 + rando2[data_aug,21])\n        tras_21_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_21_y_n),exp_std[1],exp_mean[1]))\n        \n        # 22 vertical traslation + vertical dilatation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_22_y_n = (cs(x_n) + rando[data_aug,22]) * d[data_aug,22]\n        tras_22_y_n[pos_ran] = tras_22_y_n[pos_ran] * (1 + rando[data_aug,22])\n        tras_22_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_22_y_n),exp_std[1] * d[data_aug,22],exp_mean[1] + exp_std[1] * rando[data_aug,22] * d[data_aug,22]))\n        \n        # 23 vertical traslation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_23_y_n = cs(x_n) + rando[data_aug,23] \n        tras_23_y_n[pos_ran] = tras_23_y_n[pos_ran] * (1 + rando[data_aug,23])\n        tras_23_y_n[pos_max] = tras_23_y_n[pos_max] * (1 + rando2[data_aug,23])\n        tras_23_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_23_y_n),exp_std[1],rando_de[23]))\n        \n        # 24 vertical traslation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_24_y_n = cs(x_n) + rando[data_aug,24] \n        tras_24_y_n[pos_ran] = tras_24_y_n[pos_ran] * (1 + rando[data_aug,24])\n        tras_24_y_n[pos_min] = tras_24_y_n[pos_min] * (1 - rando2[data_aug,24])  \n        tras_24_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_24_y_n),exp_std[1],rando_de[24]))\n        \n        # 25 vertical dilatation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_25_y_n = y_n * d[data_aug,25] \n        tras_25_y_n[pos_ran] = tras_25_y_n[pos_ran] * (1 + rando[data_aug,25])\n        tras_25_y_n[pos_max] = tras_25_y_n[pos_max] * (1 + rando2[data_aug,25])\n        tras_25_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_25_y_n),exp_std[1] * d[data_aug,25],exp_mean[1]))\n        \n        # 26 vertical dilatation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_26_y_n = y_n * d[data_aug,26]\n        tras_26_y_n[pos_ran] = tras_26_y_n[pos_ran] * (1 + rando[data_aug,26])\n        tras_26_y_n[pos_min] = tras_26_y_n[pos_min] * (1 - rando2[data_aug,26])  \n        tras_26_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_26_y_n),exp_std[1] * d[data_aug,26],exp_mean[1]))\n        \n        # 27 vertical traslation + vertical dilatation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_27_y_n = (cs(x_n) + rando[data_aug,27]) * d[data_aug,27]\n        tras_27_y_n[pos_ran] = tras_27_y_n[pos_ran] * (1 + rando[data_aug,27])\n        tras_27_y_n[pos_max] = tras_27_y_n[pos_max] * (1 + rando2[data_aug,27])\n        tras_27_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_27_y_n),exp_std[1] * d[data_aug,27],exp_mean[1] + exp_std[1] * rando[data_aug,27] * d[data_aug,27]))      \n       \n        # 28 vertical traslation + vertical dilatation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_28_y_n = (cs(x_n) + rando[data_aug,28]) * d[data_aug,28]\n        tras_28_y_n[pos_ran] = tras_28_y_n[pos_ran] * (1 + rando[data_aug,28])\n        tras_28_y_n[pos_min] = tras_28_y_n[pos_min] * (1 - rando2[data_aug,28])  \n        tras_28_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_28_y_n),exp_std[1] * d[data_aug,28],exp_mean[1] + exp_std[1] * rando[data_aug,28] * d[data_aug,28]))      \n       \n        # 29 vertical traslation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_29_y_n = cs(x_n) + rando[data_aug,29] \n        tras_29_y_n[pos_ran] = tras_29_y_n[pos_ran] * (1 + rando[data_aug,29])\n        tras_29_y_n[pos_max] = tras_29_y_n[pos_max] * (1 + rando2[data_aug,29])\n        tras_29_y_n[pos_min] = tras_29_y_n[pos_min] * (1 - rando2[data_aug,29])\n        tras_29_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_29_y_n),exp_std[1],rando_de[29]))\n        \n        # 30 vertical dilatation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_30_y_n = cs(x_n) * d[data_aug,30]\n        tras_30_y_n[pos_ran] = tras_30_y_n[pos_ran] * (1 + rando[data_aug,30])\n        tras_30_y_n[pos_max] = tras_30_y_n[pos_max] * (1 + rando2[data_aug,30])\n        tras_30_y_n[pos_min] = tras_30_y_n[pos_min] * (1 - rando2[data_aug,30])\n        tras_30_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_30_y_n),exp_std[1] * d[data_aug,30],exp_mean[1]))\n        \n        # 31 vertical traslation + vertical dilatation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_31_y_n = (cs(x_n) + rando[data_aug,31]) * d[data_aug,31]\n        tras_31_y_n[pos_ran] = tras_31_y_n[pos_ran] * (1 + rando[data_aug,31])\n        tras_31_y_n[pos_max] = tras_31_y_n[pos_max] * (1 + rando2[data_aug,31])\n        tras_31_y_n[pos_min] = tras_31_y_n[pos_min] * (1 - rando2[data_aug,31])  \n        tras_31_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_31_y_n),exp_std[1] * d[data_aug,31],exp_mean[1] + exp_std[1] * rando[data_aug,31] * d[data_aug,31]))      \n        \n    \n        # compute the curvematching for the three transformations\n        a = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_y_de_n, numberOfBootstrapVariations=1)\n        b = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_2_y_de_n, numberOfBootstrapVariations=1)\n        c = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_3_y_de_n, numberOfBootstrapVariations=1)\n        e = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_4_y_de_n, numberOfBootstrapVariations=1)        \n        f = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_5_y_de_n, numberOfBootstrapVariations=1)\n        g = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_6_y_de_n, numberOfBootstrapVariations=1)\n        h = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_7_y_de_n, numberOfBootstrapVariations=1)\n        i = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_8_y_de_n, numberOfBootstrapVariations=1)\n        j = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_9_y_de_n, numberOfBootstrapVariations=1)        \n        k = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_10_y_de_n, numberOfBootstrapVariations=1)\n        l = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_11_y_de_n, numberOfBootstrapVariations=1)\n        m = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_12_y_de_n, numberOfBootstrapVariations=1)\n        n = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_13_y_de_n, numberOfBootstrapVariations=1)\n        o = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_14_y_de_n, numberOfBootstrapVariations=1)\n        p = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_15_y_de_n, numberOfBootstrapVariations=1)\n        \n        q = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_16_y_de_n, numberOfBootstrapVariations=1)\n        r = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_17_y_de_n, numberOfBootstrapVariations=1)\n        s = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_18_y_de_n, numberOfBootstrapVariations=1)\n        t = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_19_y_de_n, numberOfBootstrapVariations=1)        \n        u = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_20_y_de_n, numberOfBootstrapVariations=1)\n        v = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_21_y_de_n, numberOfBootstrapVariations=1)\n        w = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_22_y_de_n, numberOfBootstrapVariations=1)\n        x = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_23_y_de_n, numberOfBootstrapVariations=1)\n        y = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_24_y_de_n, numberOfBootstrapVariations=1)        \n        z = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_25_y_de_n, numberOfBootstrapVariations=1)\n        a2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_26_y_de_n, numberOfBootstrapVariations=1)\n        b2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_27_y_de_n, numberOfBootstrapVariations=1)\n        c2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_28_y_de_n, numberOfBootstrapVariations=1)\n        e2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_29_y_de_n, numberOfBootstrapVariations=1)\n        f2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_30_y_de_n, numberOfBootstrapVariations=1)\n        g2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_31_y_de_n, numberOfBootstrapVariations=1)\n                \n        \n        # the output of the curvematching is in json format\n        a_1 = json.loads(a)  # 1 trasl\n        b_1 = json.loads(b)  # 2 dil\n        c_1 = json.loads(c)  # 3 trasl + dil\n        e_1 = json.loads(e)  # 4 max as outlier\n        f_1 = json.loads(f)  # 5 trasl + max as outlier\n        g_1 = json.loads(g)  # 6 dil + max as outlier\n        h_1 = json.loads(h)  # 7 trasl + dil + max as outlier\n        i_1 = json.loads(i)  # 8 min as outlier\n        j_1 = json.loads(j)  # 9 trasl + min as outlier\n        k_1 = json.loads(k)  # 10 dil + min as outlier\n        l_1 = json.loads(l)  # 11 trasl + dil + min as outlier\n        m_1 = json.loads(m)  # 12 min and max as outliers\n        n_1 = json.loads(n)  # 13 trasl +  min and max as outliers\n        o_1 = json.loads(o)  # 14 dil + min and max as outliers\n        p_1 = json.loads(p)  # 15 trasl + dil + min and max as outliers\n        \n        q_1 = json.loads(q)  # 16 random point\n        r_1 = json.loads(r)  # 17 trasl + random point\n        s_1 = json.loads(s)  # 18 dil + random point\n        t_1 = json.loads(t)  # 19 max as outlier + random point\n        u_1 = json.loads(u)  # 20 min as outlier + random point\n        v_1 = json.loads(v)  # 21 max and min as outliers + random point\n        w_1 = json.loads(w)  # 22 trasl + dil + random point\n        x_1 = json.loads(x)  # 23 trasl + max as outlier + random point\n        y_1 = json.loads(y)  # 24 trasl + min as outlier + random point\n        z_1 = json.loads(z)  # 25 dil + max as outlier + random point\n        a2_1 = json.loads(a2)  # 26 dil + min as outlier + random point\n        b2_1 = json.loads(b2)  # 27 trasl + dil + max as outlier + random point\n        c2_1 = json.loads(c2)  # 28 trasl + dil + min as outlier + random point\n        e2_1 = json.loads(e2)  # 29 trasl + min and max as outliers + random point\n        f2_1 = json.loads(f2)  # 30 dil + min and max as outliers + random point\n        g2_1 = json.loads(g2)  # 31 trasl + dil + min and max as outliers + random point\n\n        \n        # the final format is a dataframe\n        df = df.append(pd.DataFrame([a_1]))\n        df = df.append(pd.DataFrame([b_1]))\n        df = df.append(pd.DataFrame([c_1]))\n        df = df.append(pd.DataFrame([e_1]))\n        df = df.append(pd.DataFrame([f_1]))\n        df = df.append(pd.DataFrame([g_1]))\n        df = df.append(pd.DataFrame([h_1]))\n        df = df.append(pd.DataFrame([i_1]))\n        df = df.append(pd.DataFrame([j_1]))\n        df = df.append(pd.DataFrame([k_1]))\n        df = df.append(pd.DataFrame([l_1]))\n        df = df.append(pd.DataFrame([m_1]))\n        df = df.append(pd.DataFrame([n_1]))\n        df = df.append(pd.DataFrame([o_1]))\n        df = df.append(pd.DataFrame([p_1]))\n        \n        df = df.append(pd.DataFrame([q_1]))\n        df = df.append(pd.DataFrame([r_1]))\n        df = df.append(pd.DataFrame([s_1]))\n        df = df.append(pd.DataFrame([t_1]))\n        df = df.append(pd.DataFrame([u_1]))\n        df = df.append(pd.DataFrame([v_1]))\n        df = df.append(pd.DataFrame([w_1]))\n        df = df.append(pd.DataFrame([x_1]))\n        df = df.append(pd.DataFrame([y_1]))\n        df = df.append(pd.DataFrame([z_1]))\n        df = df.append(pd.DataFrame([a2_1]))\n        df = df.append(pd.DataFrame([b2_1]))\n        df = df.append(pd.DataFrame([c2_1]))\n        df = df.append(pd.DataFrame([e2_1]))\n        df = df.append(pd.DataFrame([f2_1]))\n        df = df.append(pd.DataFrame([g2_1]))\n        \n        # 1 vertical traslation\n        ls_f1.append(rando_de[1])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 2 vertical dilatation\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,2])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 3 vertical traslation + vertical dilatation\n        ls_f1.append(rando_de[3] * d[data_aug,3])\n        ls_f2.append(d[data_aug,3])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 4 max as outlier\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,4])\n        ls_f4.append(0)\n        ls_f5.append(0)\n       \n        # 5 vertical traslation + max as outlier\n        ls_f1.append(rando_de[5])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,5])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 6 vertical dilatation + max as outlier\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,6])\n        ls_f3.append(rando2[data_aug,6])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 7 vertical traslation + vertical dilatation + max as outlier\n        ls_f1.append(rando_de[7] * d[data_aug,7])\n        ls_f2.append(d[data_aug,7])\n        ls_f3.append(rando2[data_aug,7])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 8 min as outlier\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,8])\n        ls_f5.append(0)\n        \n        # 9 vertical traslation + min as outlier\n        ls_f1.append(rando_de[9])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,9])\n        ls_f5.append(0)\n        \n        # 10 vertical dilatation + min as outlier\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,10])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,10])\n        ls_f5.append(0)\n        \n        # 11 vertical traslation + vertical dilatation + min as outlier\n        ls_f1.append(rando_de[11] * d[data_aug,11])\n        ls_f2.append(d[data_aug,11])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,11])\n        ls_f5.append(0)\n        \n        # 12 min and max as outliers\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,12])\n        ls_f4.append(-rando2[data_aug,12])\n        ls_f5.append(0)\n        \n        # 13 vertical traslation + min and max as outliers\n        ls_f1.append(rando_de[13])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,13])\n        ls_f4.append(-rando2[data_aug,13])\n        ls_f5.append(0)\n        \n        # 14 vertical dilatation + min and max as outliers\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,14])\n        ls_f3.append(rando2[data_aug,14])\n        ls_f4.append(-rando2[data_aug,14])\n        ls_f5.append(0)\n        \n        # 15 vertical traslation + vertical dilatation + min and max as outliers\n        ls_f1.append(rando_de[15] * d[data_aug,15])\n        ls_f2.append(d[data_aug,15])\n        ls_f3.append(rando2[data_aug,15])\n        ls_f4.append(-rando2[data_aug,15])\n        ls_f5.append(0)\n        \n        # 16 random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,16])\n        \n        # 17 vertical traslation + random point\n        ls_f1.append(rando_de[17])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,17])\n        \n        # 18 vertical dilatation + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,18])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,18])\n        \n        # 19 max as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,19])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,19])\n        \n        # 20 min as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,20])\n        ls_f5.append(rando[data_aug,20])\n        \n        # 21 max and min as outliers + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,21])\n        ls_f4.append(-rando2[data_aug,21])\n        ls_f5.append(rando[data_aug,21])\n        \n        # 22 vertical traslation + vertical dilatation + random point\n        ls_f1.append(rando_de[22] * d[data_aug,22])\n        ls_f2.append(d[data_aug,22])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,22])\n        \n        # 23 vertical traslation + max as outlier + random point\n        ls_f1.append(rando_de[23])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,23])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,23])\n        \n        # 24 vertical traslation + min as outlier + random point\n        ls_f1.append(rando_de[24])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,24])\n        ls_f5.append(rando[data_aug,24])\n        \n        # 25 vertical dilatation + max as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,25])\n        ls_f3.append(rando2[data_aug,25])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,25])\n        \n        # 26 vertical dilatation + min as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,26])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,26])\n        ls_f5.append(rando[data_aug,26])\n        \n        # 27 vertical traslation + vertical dilatation + max as outlier + random point\n        ls_f1.append(rando_de[27] * d[data_aug,27])\n        ls_f2.append(d[data_aug,27])\n        ls_f3.append(rando2[data_aug,27])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,27])\n        \n        # 28 vertical traslation + vertical dilatation + min as outlier + random point\n        ls_f1.append(rando_de[28] * d[data_aug,28])\n        ls_f2.append(d[data_aug,28])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,28])\n        ls_f5.append(rando[data_aug,28])\n        \n        # 29 vertical traslation + min and max as outliers + random point\n        ls_f1.append(rando_de[29])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,29])\n        ls_f4.append(-rando2[data_aug,29])\n        ls_f5.append(rando[data_aug,29])\n        \n        # 30 vertical dilatation + min and max as outliers + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,30])\n        ls_f3.append(rando2[data_aug,30])\n        ls_f4.append(-rando2[data_aug,30])\n        ls_f5.append(rando[data_aug,30])\n        \n        # 31 vertical traslation + vertical dilatation + min and max as outliers + random point\n        ls_f1.append(rando_de[31] * d[data_aug,31])\n        ls_f2.append(d[data_aug,31])\n        ls_f3.append(rando2[data_aug,31])\n        ls_f4.append(-rando2[data_aug,31])\n        ls_f5.append(rando[data_aug,31])\n        \n    # add the new features as columns    \n    features1 = \"vert_trans_f\"\n    features2 = \"dil_vert_f\"\n    features3 = \"outlier on the max\"\n    features4 = \"outlier on the min\"\n    features5 = \"modfication on a single random point\"\n    \n    df[features1] = ls_f1\n    df[features2] = ls_f2\n    df[features3] = ls_f3\n    df[features4] = ls_f4\n    df[features5] = ls_f5\n    \n    df = df.reset_index()\n    df = df.drop([\"error\",\"index\"],axis=1)\n\n    # if the curvematching yields a wrong result, drop it\n    ind = df[df[\"score\"] < 0].index\n    df = df.drop(ind,axis = 0)\n    \n    # reorder the indexes of the dataframe\n    df = df.reset_index()\n    df = df.drop([\"index\"],axis=1) \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:52:21.278928Z","iopub.execute_input":"2022-05-22T15:52:21.279228Z","iopub.status.idle":"2022-05-22T15:52:21.426352Z","shell.execute_reply.started":"2022-05-22T15:52:21.279196Z","shell.execute_reply":"2022-05-22T15:52:21.425129Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"aaa2 = prova(1061,2)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T16:25:53.068997Z","iopub.execute_input":"2022-05-22T16:25:53.069282Z","iopub.status.idle":"2022-05-22T16:26:07.746230Z","shell.execute_reply.started":"2022-05-22T16:25:53.069252Z","shell.execute_reply":"2022-05-22T16:26:07.745465Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# multiple estraction from \tlaminar burning velocity measurement\n# num_id = the number of experiments that we want to extract from the db \n# num_data_aug = it is the number of desired data augmentation (notice that is always mulplied by 3)\n# Notice that this function will call many times the function laminar_burning_velocity_ex and it will\n# store only the dataframe that follow the following rules:\n# - the experiment must have these two columns:  'equivalence ratio' and 'laminar burning velocity'\n# - result from the curve matching must be not negative\n# - the used units are 'unitless' and 'cm/s' respectively for the 'equivalence ratio' and 'laminar burning velocity'\n# All those db results that don't follow these rules,they will be dropped\ndef laminar_burning_velocity_multiple_ex(num_id,num_data_aug):\n    \n    ex = db.filterDatabase(model_name='Experiment', experiment_type = 'laminar burning velocity measurement')\n\n    ex_id = [ex[i].id for i in range(0,len(ex))]\n    count = 0\n    df_list = []\n    \n    for i in ex_id:\n        \n        print(i)\n        df = prova(i,num_data_aug)# laminar_burning_velocity_ex(i,num_data_aug)\n\n        # drop the empty/irregular dataframe\n        if df.empty:\n            print('DataFrame is empty!')\n        else:\n            count = count + 1\n            df_list.append(df)\n            \n        if count == num_id:\n            break\n       \n    # concatenate the dataframes\n    df_tot = pd.concat(df_list)\n    \n    # reset the indexes\n    df_tot = df_tot.reset_index()\n    df_tot = df_tot.drop([\"index\"],axis=1)\n    \n    # write a csv\n    df_tot.to_csv('laminar_burning_velocity.csv', index=False)\n    \n    # to read a csv\n    # df2 = pd.read_csv('laminar_burning_velocity.csv')\n    \n    return df_tot\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:57:36.102933Z","iopub.execute_input":"2022-05-22T15:57:36.103204Z","iopub.status.idle":"2022-05-22T15:57:36.111838Z","shell.execute_reply.started":"2022-05-22T15:57:36.103174Z","shell.execute_reply":"2022-05-22T15:57:36.110466Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"aaa = laminar_burning_velocity_multiple_ex(15,15)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:57:39.474205Z","iopub.execute_input":"2022-05-22T15:57:39.474478Z","iopub.status.idle":"2022-05-22T16:21:19.525164Z","shell.execute_reply.started":"2022-05-22T15:57:39.474452Z","shell.execute_reply":"2022-05-22T16:21:19.524571Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt   #Data visualisation libraries \nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline\n\nSciExp = pd.read_csv('../input/laminar1/laminar_burning_velocity (1).csv')\nSciExp.head()\nSciExp.info()\nSciExp.describe()\nSciExp.columns\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:28:01.70906Z","iopub.execute_input":"2022-05-21T15:28:01.709532Z","iopub.status.idle":"2022-05-21T15:28:01.773882Z","shell.execute_reply.started":"2022-05-21T15:28:01.7095Z","shell.execute_reply":"2022-05-21T15:28:01.772822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# graphics \n# sns.heatmap(SciExp.corr())","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:58:49.59401Z","iopub.execute_input":"2022-05-18T14:58:49.594334Z","iopub.status.idle":"2022-05-18T14:58:49.958651Z","shell.execute_reply.started":"2022-05-18T14:58:49.594302Z","shell.execute_reply":"2022-05-18T14:58:49.95772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vert.trans.f regression\n# we are trying to determine whether the dataset that we created can be used for regression or not \nX = SciExp[['score','d0L2','d1L2','d0Pe','d1Pe','shift','vert.trans.f']]\n\nX = X[X['vert.trans.f'] != 0]\n#X = X[['score','d0L2','d1L2','d0Pe','d1Pe','shift']]\nX[['vert_trans_f']] = X[['vert.trans.f']]\nX = X.drop(['vert.trans.f'], axis= 1)\n\nlm = ols(\"vert_trans_f ~ score + d0L2 + d1L2 + d0Pe + d1Pe + shift\", data=X).fit()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:16:31.837108Z","iopub.execute_input":"2022-05-21T15:16:31.837953Z","iopub.status.idle":"2022-05-21T15:16:31.864667Z","shell.execute_reply.started":"2022-05-21T15:16:31.83791Z","shell.execute_reply":"2022-05-21T15:16:31.863515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# check on the outliers with the Cook's distance and the relative influence\n\nout_d = [True] #just to enter in the do-while\nwhile any(out_d):\n    \n    lm = ols(\"vert_trans_f ~ score + d0L2 + d1L2 + d0Pe + d1Pe + shift\", data=X).fit()\n    \n    fig = sm.graphics.influence_plot(lm, criterion=\"cooks\")\n    fig.tight_layout(pad=1.0)\n\n    lm_cooksd = lm.get_influence().cooks_distance[0]\n\n    #print(lm_cooksd)\n    n = len(X[\"vert_trans_f\"])\n\n    # calculate critical d\n    critical_d = 4/n\n    print('Critical Cooks distance:', critical_d)\n\n    # identification of potential outliers with leverage\n    out_d = lm_cooksd > critical_d\n\n    # output potential outliers with leverage\n  #  print(X.index[out_d], \"\\n\",lm_cooksd[out_d])\n\n    X = X.drop(X[out_d].index, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:16:36.164922Z","iopub.execute_input":"2022-05-21T15:16:36.165282Z","iopub.status.idle":"2022-05-21T15:16:40.041663Z","shell.execute_reply.started":"2022-05-21T15:16:36.165245Z","shell.execute_reply":"2022-05-21T15:16:40.040644Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = X[['vert_trans_f']]\nX = X.drop(['vert_trans_f'], axis= 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:51:28.907971Z","iopub.execute_input":"2022-05-21T15:51:28.908814Z","iopub.status.idle":"2022-05-21T15:51:28.91874Z","shell.execute_reply.started":"2022-05-21T15:51:28.908771Z","shell.execute_reply":"2022-05-21T15:51:28.917961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_pred = lm.predict()\nplt.figure(figsize=(12,6))\nplt.plot(X[\"d1L2\"],X[\"vert_trans_f\"],'o')\nplt.plot(X[\"d1L2\"],lm_pred,'o',linewidth=2)\n#score + d0L2 + d1L2 + d0Pe + d1Pe + shift","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:09:44.086835Z","iopub.execute_input":"2022-05-21T15:09:44.087162Z","iopub.status.idle":"2022-05-21T15:09:44.321889Z","shell.execute_reply.started":"2022-05-21T15:09:44.087128Z","shell.execute_reply":"2022-05-21T15:09:44.320892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shapiro test to check normality on the residuals\n\nfrom scipy import stats\nshapiro_test = stats.shapiro(lm.resid)\nshapiro_test","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:20:19.931485Z","iopub.execute_input":"2022-05-19T16:20:19.931777Z","iopub.status.idle":"2022-05-19T16:20:19.93819Z","shell.execute_reply.started":"2022-05-19T16:20:19.931745Z","shell.execute_reply":"2022-05-19T16:20:19.937578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatterplot with residuals and predictions following the model without outliers\n\nresiduals = lm.resid\npredictions = lm.predict(X)\nX[\"predicted\"] = lm.predict(X)\nX[\"residuals\"] = lm.resid\nsns.scatterplot(data=X, x=\"predicted\", y=\"residuals\")\nplt.axhline(y=0)\nplt.axhline(np.mean(residuals))\n# from sklearn.linear_model import LinearRegression\n# from yellowbrick.regressor import ResidualsPlot\n\n# # Instantiate and fit the visualizer\n# model = LinearRegression()\n# visualizer_residuals = ResidualsPlot(model)\n# visualizer_residuals.fit(X,y)\n# visualizer_residuals.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:20:21.609039Z","iopub.execute_input":"2022-05-19T16:20:21.609688Z","iopub.status.idle":"2022-05-19T16:20:21.828262Z","shell.execute_reply.started":"2022-05-19T16:20:21.609654Z","shell.execute_reply":"2022-05-19T16:20:21.827523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.losses import MeanSquaredLogarithmicError\ntfk = tf.keras","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:56:13.97456Z","iopub.execute_input":"2022-05-21T15:56:13.974927Z","iopub.status.idle":"2022-05-21T15:56:13.983104Z","shell.execute_reply.started":"2022-05-21T15:56:13.974885Z","shell.execute_reply":"2022-05-21T15:56:13.981884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_units1 = 160\nhidden_units2 = 480\nhidden_units3 = 256\n\n# Creating model using the Sequential in tensorflow\ndef build_model_using_sequential():\n  model = Sequential([\n    Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n    Dropout(0.2),\n    Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n    Dropout(0.2),\n    Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n    Dense(1, kernel_initializer='normal', activation='linear')\n  ])\n  return model\n# build the model\nmodel = build_model_using_sequential()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:54:28.13221Z","iopub.execute_input":"2022-05-21T15:54:28.132577Z","iopub.status.idle":"2022-05-21T15:54:28.211106Z","shell.execute_reply.started":"2022-05-21T15:54:28.132538Z","shell.execute_reply":"2022-05-21T15:54:28.210149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss function\nmsle = MeanSquaredLogarithmicError()\nlearning_rate = 0.01\n\nmodel.compile(\n    loss=msle, \n    optimizer=Adam(learning_rate=learning_rate), \n    metrics=[msle]\n)\n\n# train the model\nhistory = model.fit(\n    X_train.values, \n    y_train.values, \n    epochs=200, \n    batch_size=64,\n    validation_split=0.2,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:02:42.003599Z","iopub.execute_input":"2022-05-21T16:02:42.00392Z","iopub.status.idle":"2022-05-21T16:02:44.264839Z","shell.execute_reply.started":"2022-05-21T16:02:42.003887Z","shell.execute_reply":"2022-05-21T16:02:44.264108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test['prediction'] = model.predict(X_test)\nX_test['target'] = y_test","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:05:36.431412Z","iopub.execute_input":"2022-05-21T16:05:36.432582Z","iopub.status.idle":"2022-05-21T16:05:36.438524Z","shell.execute_reply.started":"2022-05-21T16:05:36.432492Z","shell.execute_reply":"2022-05-21T16:05:36.437541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:05:44.26181Z","iopub.execute_input":"2022-05-21T16:05:44.262451Z","iopub.status.idle":"2022-05-21T16:05:44.294195Z","shell.execute_reply.started":"2022-05-21T16:05:44.262397Z","shell.execute_reply":"2022-05-21T16:05:44.292997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://stackoverflow.com/questions/66177666/how-do-you-create-a-design-matrix-in-python","metadata":{}}]}