{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install SciExpeM-API\n!pip install SciExpeM-API --upgrade\nfrom SciExpeM_API.SciExpeM import SciExpeM","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os \nfrom scipy.interpolate import CubicSpline\nimport matplotlib.pyplot as plt\nimport random\nrandom.seed(2)\nimport pandas as pd\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:47:08.592773Z","iopub.execute_input":"2022-05-27T12:47:08.593169Z","iopub.status.idle":"2022-05-27T12:47:08.606844Z","shell.execute_reply.started":"2022-05-27T12:47:08.593102Z","shell.execute_reply":"2022-05-27T12:47:08.605745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check if the connection is working or not\n\ndb = SciExpeM(username='manuel.peracci', password='mdp2022_',verify=False,warning=False)\ndb.testConnection(verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation of the Curve Matching\n\n# we want to see if it is possibile to recognise a specific trasformation from the results of the Curve Matching.\n# To apply the Curve Matching to two curves, we rely on the already implemented functions inside SciExpeM.\n# In order to test it the general idea was to create a dataset collecting the results of the curve matching and \n# the features of the curve matching. We decided to go for the follwoing new features: vertical translation, vertical dilation,\n# vertical translation of the max,vertical translation of the min and vertical translation of the single random point.\n# The first two features are strongly linked to the types of distances used to compute the curve matching, while the rest\n# are useful to understand the robustness of this model to a possibile mistake in a single point. So, the third and fourth can\n# be seen as an outliers, while the last one is the error in a random point in the curve.\n# Notice that the general idea was to build the most general dataset possibile keeping into account the fact that \n# it could be as template or starting point for other works.\n\n# Our aim was to perform classification, but we decided to create a dataset for regression, allowing all the possibile \n# combinations of the five new features. For instance, if we only want a vertical traslation, we will set it to\n# a certain value and we will leave the rest of the new features to zero.\n# This is a design choise in order to allow to perform both tasks (classification and regression). \n# In fact, it's possible to pass from a dataset for regression to another one for classification, defining a number of class equal \n# to the number of new features and definying the ownership to a class when that specific feature is different from zero. \n# To be more precise, in this way are going to create a dataset for multilabel classification, so the observations can\n# belong to more than one class.\n# We tried this type of classification, but it didn't help in creating a classifer with good performance. So, we tried\n# a simpler approach creating a multiclass singlelabel dataset starting from the one with all the combinations and we kept\n# only the observations with only one trasfromation applied. Furthermore, the machine learning approach for this type of classifer\n# can be found in the other notebook: curve_matching_machine_learning.\n# Finally, notice that for the profiling of the data, we used the dataset for regression. ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A little recap about CurveMatching state-of-the-arts\n\n# As the model name says it tries to match two functions and basically it tries to understand how much similar these two functions are. \n# The original data used comes from the combustion experiments, while the other one (usually) comes from a functions created by a model.\n# Notice that each experiment has many different features: reactors, fuel, experiment type, etc. For a better understaing of the features,\n# we suggest to work directly on the webpage of SciExpeM https://sciexpem.chem.polimi.it (credentials are mandatory) and to read the following \n# articles: https://www.frontiersin.org/articles/10.3389/fdata.2021.663410/full\n        \n# The similarity between the functions are computed using six features: Score, d0L2, d1L2, d0Pe, d1Pe and shift. The first one is the \n# mean of the fololwing distances and it is the main output. The folowing two are L2 norms of the two functions and their derivatives, respectively. \n# The following two are Pearson norms that compute the correlations between two functions and their derivatives, respectively. \n# The last one is involved in the alignment dissimilarity between the two functions. \n# For further details, it is possibile to check the following article: https://www.sciencedirect.com/science/article/pii/S0010218016300360\n\n# About the api, there are two main functions that are used for the creation of the dataset: filterDatabase() and executeCurveMatching().\n# The first one download the data using the arguments of the function as filter (it will gives as output a class with all the data), \n# while the second one executes the curve matching between points of two curves and compute the previous six features.\n# For further details, it's possible to check the documentation of the api.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the main function for the construction of the dataset: it downloads the data of a specific experiment, it does data augmentation\n# using random number and applying specific trasformations, then it asks for the result of the curve matching and finally it creates a pandas table\n# with the features of the curve matching and the new ones.\n# Notice that this function is really general and it can be used for different type of experiments, reactors and so on. \n# Notice also that the right X and Y for the functions are found using the properties pairs of the api class (for different type of experiments\n# they can be different)\n# This function is called multiple times from the function laminar_burning_velocity_multiple_ex, which will collect all the new obserations.\n# Finally, remember that this function will create all the possibile combinations of the features, which are 31 and they are specified inside \n# the code.\n# To sum up the function: \n# - we normalized all the data coming from the database, saving the mean and the standard deviation\n# - we created random matrices to use for data augmentation (notice that we used as units the standard deviation to make it the most general possibile)\n# - we created the original function using a cubicspline\n# - we applied the desired trasformations obtaining new functions\n# - we properly denormalized the new functions \n# - we applied the curvemathing function\n# - we created a pandas table with all the observations \n# - we iterated this procedure as much as desired (ospecified in the argument num_data_aug)\n\n# this fuction will help to detect in the x-axis because later we will call the CubicSpline and\n# it doesn't work well if there are duplicates \ndef list_duplicates(seq):\n  seen = set()\n  seen_add = seen.add\n  seen_twice = set( x for x in seq if x in seen or seen_add(x) )\n  return list( seen_twice )\n\ndef normalizeArray(ar):\n    return (ar - np.mean(ar))/(np.std(ar))\n\ndef deNorm(ar,std,mean):\n    return ar * std + mean \n\ndef random_matrix(data_aug,a,b,arr): \n    listoflists = []\n    for ele in range(0,data_aug):\n        a_list = []\n        for ala in range(0,32):\n            a_list.append(random.uniform(a*np.std(arr),b*np.std(arr)))\n        listoflists.append(a_list)\n    return np.array(listoflists)\n\n# it extracts some data from the experiment type laminar burning velocity measurement\n# ex_id = it is the experiment identifier\n# num_data_aug = it is the number of desired data augmentation (notice that is always mulplied by 3)\n# chemModel_id = the model identifier which can be omitted \ndef prova(ex_id,num_data_aug,chemModel_id = 0): \n    \n    if(chemModel_id == 0):\n        ex = db.filterDatabase(model_name='Execution', experiment__id=ex_id)\n    else:\n        ex = db.filterDatabase(model_name='Execution', experiment__id=ex_id, chemModel__id=chemModel_id)\n        \n    #if it is empty, return an empty df\n    if not ex:\n        return pd.DataFrame()\n    else:\n        ex = ex[0]\n    \n    # find the right data_columns\n    exp =[ex.experiment.pairs[0][\"x\"],ex.experiment.pairs[0][\"y\"]]\n    \n    if(exp[0].units != 'unitless' or exp[1].units != 'cm/s'):\n        print(\"Warning: different units\")\n        return pd.DataFrame()\n    \n    xx = np.round(exp[0].data,7)\n    yy = np.round(exp[1].data,7)\n    dupl = list_duplicates(xx)\n\n    # In case of duplicates, we decided to leave only the first one \n    # It caused problems to the CubicSpline function\n    if dupl:\n        for i in list_duplicates(xx):\n\n            c = np.where(xx == np.round(i,7))[0]\n            c = c[1:np.size(c)]\n\n            yy = np.delete(yy,c)\n            xx = np.delete(xx,c)\n            \n        dupl = list_duplicates(xx)  \n        # if there are still duplicates, send a warning \n        if dupl:\n            print(\"Warning: duplicates found\")\n            print(\"duplicates:\",dupl)\n    \n    # compute mean and variance that there will come in handy later \n    exp_mean = np.column_stack((np.mean(xx),np.mean(yy)))[0]\n    exp_std = np.column_stack((np.std(xx),np.std(yy)))[0]\n    \n    # normalize the values on the x-axis and y-axis\n    orig = np.column_stack((normalizeArray(np.array(xx)),normalizeArray(np.array(yy))))\n    \n    # order the values for the CubicSpline function \n    orig_s = orig[np.argsort(orig[:,0])].T\n\n    # compute some random values that will be used either to traslate or dilate the original function\n    # these values are based on the standard deviation of x \n    rando = random_matrix(num_data_aug,-1,1,orig_s[1]) #np.array([random.uniform((-1)*np.std(orig_s[1]),np.std(orig_s[1])) for ele in range(0,num_data_aug)])  \n    d = random_matrix(num_data_aug,0.5,2,orig_s[1]) # np.array([random.uniform((0.5)*np.std(orig_s[1]),2*np.std(orig_s[1])) for ele in range(0,num_data_aug)])\n    rando2 = random_matrix(num_data_aug,0.1,1,orig_s[1]) # np.array([random.uniform(0.1,1*np.std(orig_s[1])) for ele in range(0,num_data_aug)])\n    \n    # in order not to use always the same points on the x-axis for the curvematching, we came up with this solution:\n    # we split the array on the x-axis into sub-arrays and on each of them we picked a random value \n    # we used all these new random values as new points on the x-axis\n    # notice that we chose this because we wanted to keep some kind of uniform distribution \n    # for the points and also the same number of points\n    newarr = np.array_split(orig_s[0], len(orig_s[0]))\n    x_n = []\n    for ele in range(0,len(newarr)):\n        if ele == (len(newarr) - 1): break\n        x_n.append(random.uniform(newarr[ele],newarr[ele+1])[0])\n    \n    # we decided out of coherence to use the cubic interpolation because it was used also in the curve matching algorithm\n    cs = CubicSpline(orig_s[0],orig_s[1]) \n    ls_f1 = []\n    ls_f2 = []\n    ls_f3 = []\n    ls_f4 = []\n    ls_f5 = []\n    df = pd.DataFrame(columns=[\"score\",\"error\",\"d0L2\",\"d1L2\",\"d0Pe\",\"d1Pe\",\"shift\"])\n     \n    \n    # data augmentation on the curve created from the original experiment \n    # notice that everything is being normalized because the experiments have different scales \n    # and then everything is denormalized in order to store correctly the data on the dataframe\n    for data_aug in range(0,num_data_aug):  \n        \n        rando_de = deNorm(rando[data_aug],exp_std[1],exp_mean[1])\n        \n        # 1 vertical traslation\n        x_de_n = np.ndarray.tolist(deNorm(np.array(x_n),exp_std[0],exp_mean[0]))\n        y_n = cs(x_n)\n        tras_y_n = cs(x_n) + rando[data_aug,1]\n        y_de_n = np.ndarray.tolist(deNorm(np.array(y_n),exp_std[1],exp_mean[1]))\n        tras_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_y_n),exp_std[1],rando_de[1]))\n\n        # 2 vertical dilatation\n        tras_2_y_n = cs(x_n) * d[data_aug,2]\n        tras_2_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_2_y_n),exp_std[1] * d[data_aug,2],exp_mean[1]))\n\n        # 3 vertical traslation + vertical dilatation\n        tras_3_y_n = (cs(x_n) + rando[data_aug,3]) * d[data_aug,3]\n        tras_3_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_3_y_n),exp_std[1] * d[data_aug,3],exp_mean[1] + exp_std[1] * rando[data_aug,3] * d[data_aug,3]))\n       \n        # 4 max as outlier -> incremented with a random number between 0 and 1 std\n        pos_max = np.argmax(y_n)\n        tras_4_y_n = y_n\n        tras_4_y_n[pos_max] = tras_4_y_n[pos_max] * (1 + rando2[data_aug,4])\n        tras_4_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_4_y_n),exp_std[1],exp_mean[1]))\n        \n        # 5 vertical traslation + max as outlier\n        tras_5_y_n = y_n + rando[data_aug,5]\n        tras_5_y_n[pos_max] = tras_5_y_n[pos_max] * (1 + rando2[data_aug,5])\n        tras_5_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_5_y_n),exp_std[1],rando_de[5]))\n        \n        # 6 vertical dilatation + max as outlier\n        tras_6_y_n = y_n * d[data_aug,6]\n        tras_6_y_n[pos_max] = tras_6_y_n[pos_max] * (1 + rando2[data_aug,6])\n        tras_6_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_6_y_n),exp_std[1] * d[data_aug,6],exp_mean[1]))\n        \n        # 7 vertical traslation + vertical dilatation + max as outlier\n        tras_7_y_n = (cs(x_n) + rando[data_aug,7]) * d[data_aug,7]\n        tras_7_y_n[pos_max] = tras_7_y_n[pos_max] * (1 + rando2[data_aug,7])\n        tras_7_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_7_y_n),exp_std[1] * d[data_aug,7],exp_mean[1] + exp_std[1] * rando[data_aug,7] * d[data_aug,7]))\n      \n        # 8 min as outlier -> decreased by a random number between 0 and 1 std\n        pos_min = np.argmin(y_n)\n        tras_8_y_n = y_n\n        tras_8_y_n[pos_min] = tras_8_y_n[pos_min] * (1 - rando2[data_aug,8])\n        tras_8_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_8_y_n),exp_std[1],exp_mean[1]))\n        \n        # 9 vertical traslation + min as outlier\n        tras_9_y_n = y_n + rando[data_aug,9]\n        tras_9_y_n[pos_min] = tras_9_y_n[pos_min] * (1 - rando2[data_aug,9])\n        tras_9_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_9_y_n),exp_std[1],rando_de[9]))\n        \n        # 10 vertical dilatation + min as outlier\n        tras_10_y_n = y_n * d[data_aug,10]\n        tras_10_y_n[pos_min] = tras_10_y_n[pos_min] * (1 - rando2[data_aug,10])\n        tras_10_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_10_y_n),exp_std[1] * d[data_aug,10],exp_mean[1]))\n        \n        # 11 vertical traslation + vertical dilatation + min as outlier\n        tras_11_y_n = (cs(x_n) + rando[data_aug,11]) * d[data_aug,11]\n        tras_11_y_n[pos_min] = tras_11_y_n[pos_min] * (1 - rando2[data_aug,11])\n        tras_11_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_11_y_n),exp_std[1] * d[data_aug,11],exp_mean[1] + exp_std[1] * rando[data_aug,11] * d[data_aug,11]))\n      \n        # 12 min and max as outliers \n        tras_12_y_n = y_n\n        tras_12_y_n[pos_min] = tras_12_y_n[pos_min] * (1 - rando2[data_aug,12])\n        tras_12_y_n[pos_max] = tras_12_y_n[pos_max] * (1 + rando2[data_aug,12])\n        tras_12_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_12_y_n),exp_std[1],exp_mean[1]))\n        \n        # 13 vertical traslation + min and max as outliers \n        tras_13_y_n = y_n + rando[data_aug,13]\n        tras_13_y_n[pos_min] = tras_13_y_n[pos_min] * (1 - rando2[data_aug,13])\n        tras_13_y_n[pos_max] = tras_13_y_n[pos_max] * (1 + rando2[data_aug,13])\n        tras_13_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_13_y_n),exp_std[1],rando_de[13]))\n        \n        # 14 vertical dilatation + min and max as outliers \n        tras_14_y_n = y_n * d[data_aug,14]\n        tras_14_y_n[pos_min] = tras_14_y_n[pos_min] * (1 - rando2[data_aug,14])\n        tras_14_y_n[pos_max] = tras_14_y_n[pos_max] * (1 + rando2[data_aug,14])\n        tras_14_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_14_y_n),exp_std[1] * d[data_aug,14],exp_mean[1]))\n        \n        # 15 vertical traslation + vertical dilatation + min and max as outliers\n        tras_15_y_n = (cs(x_n) + rando[data_aug,15]) * d[data_aug,15]\n        tras_15_y_n[pos_min] = tras_15_y_n[pos_min] * (1 - rando2[data_aug,15])\n        tras_15_y_n[pos_max] = tras_15_y_n[pos_max] * (1 + rando2[data_aug,15])\n        tras_15_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_15_y_n),exp_std[1] * d[data_aug,15],exp_mean[1] + exp_std[1] * rando[data_aug,15] * d[data_aug,15]))      \n       \n        # 16 random point -> random modification of a point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_16_y_n = y_n\n        tras_16_y_n[pos_ran] = tras_16_y_n[pos_ran] * (1 + rando[data_aug,16])\n        tras_16_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_16_y_n),exp_std[1],exp_mean[1]))\n        \n        # 17 vertical traslation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_17_y_n = y_n + rando[data_aug,17]\n        tras_17_y_n[pos_ran] = tras_17_y_n[pos_ran] * (1 + rando[data_aug,17])\n        tras_17_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_17_y_n),exp_std[1],rando_de[17]))\n        \n        # 18 vertical dilatation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_18_y_n = y_n * d[data_aug,18]\n        tras_18_y_n[pos_ran] = tras_18_y_n[pos_ran] * (1 + rando[data_aug,18])\n        tras_18_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_18_y_n),exp_std[1] * d[data_aug,18],exp_mean[1]))\n        \n        # 19 max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_19_y_n = y_n \n        tras_19_y_n[pos_ran] = tras_19_y_n[pos_ran] * (1 + rando[data_aug,19])\n        tras_19_y_n[pos_max] = tras_19_y_n[pos_max] * (1 + rando2[data_aug,19])\n        tras_19_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_19_y_n),exp_std[1],exp_mean[1]))\n        \n        # 20 min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_20_y_n = y_n \n        tras_20_y_n[pos_ran] = tras_20_y_n[pos_ran] * (1 + rando[data_aug,20])\n        tras_20_y_n[pos_min] = tras_20_y_n[pos_min] * (1 - rando2[data_aug,20])        \n        tras_20_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_20_y_n),exp_std[1],exp_mean[1]))\n        \n        # 21 max and min as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_21_y_n = y_n \n        tras_21_y_n[pos_ran] = tras_21_y_n[pos_ran] * (1 + rando[data_aug,21])\n        tras_21_y_n[pos_min] = tras_21_y_n[pos_min] * (1 - rando2[data_aug,21])  \n        tras_21_y_n[pos_max] = tras_21_y_n[pos_max] * (1 + rando2[data_aug,21])\n        tras_21_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_21_y_n),exp_std[1],exp_mean[1]))\n        \n        # 22 vertical traslation + vertical dilatation + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_22_y_n = (cs(x_n) + rando[data_aug,22]) * d[data_aug,22]\n        tras_22_y_n[pos_ran] = tras_22_y_n[pos_ran] * (1 + rando[data_aug,22])\n        tras_22_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_22_y_n),exp_std[1] * d[data_aug,22],exp_mean[1] + exp_std[1] * rando[data_aug,22] * d[data_aug,22]))\n        \n        # 23 vertical traslation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_23_y_n = cs(x_n) + rando[data_aug,23] \n        tras_23_y_n[pos_ran] = tras_23_y_n[pos_ran] * (1 + rando[data_aug,23])\n        tras_23_y_n[pos_max] = tras_23_y_n[pos_max] * (1 + rando2[data_aug,23])\n        tras_23_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_23_y_n),exp_std[1],rando_de[23]))\n        \n        # 24 vertical traslation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_24_y_n = cs(x_n) + rando[data_aug,24] \n        tras_24_y_n[pos_ran] = tras_24_y_n[pos_ran] * (1 + rando[data_aug,24])\n        tras_24_y_n[pos_min] = tras_24_y_n[pos_min] * (1 - rando2[data_aug,24])  \n        tras_24_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_24_y_n),exp_std[1],rando_de[24]))\n        \n        # 25 vertical dilatation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_25_y_n = y_n * d[data_aug,25] \n        tras_25_y_n[pos_ran] = tras_25_y_n[pos_ran] * (1 + rando[data_aug,25])\n        tras_25_y_n[pos_max] = tras_25_y_n[pos_max] * (1 + rando2[data_aug,25])\n        tras_25_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_25_y_n),exp_std[1] * d[data_aug,25],exp_mean[1]))\n        \n        # 26 vertical dilatation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_26_y_n = y_n * d[data_aug,26]\n        tras_26_y_n[pos_ran] = tras_26_y_n[pos_ran] * (1 + rando[data_aug,26])\n        tras_26_y_n[pos_min] = tras_26_y_n[pos_min] * (1 - rando2[data_aug,26])  \n        tras_26_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_26_y_n),exp_std[1] * d[data_aug,26],exp_mean[1]))\n        \n        # 27 vertical traslation + vertical dilatation + max as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_27_y_n = (cs(x_n) + rando[data_aug,27]) * d[data_aug,27]\n        tras_27_y_n[pos_ran] = tras_27_y_n[pos_ran] * (1 + rando[data_aug,27])\n        tras_27_y_n[pos_max] = tras_27_y_n[pos_max] * (1 + rando2[data_aug,27])\n        tras_27_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_27_y_n),exp_std[1] * d[data_aug,27],exp_mean[1] + exp_std[1] * rando[data_aug,27] * d[data_aug,27]))      \n       \n        # 28 vertical traslation + vertical dilatation + min as outlier + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_28_y_n = (cs(x_n) + rando[data_aug,28]) * d[data_aug,28]\n        tras_28_y_n[pos_ran] = tras_28_y_n[pos_ran] * (1 + rando[data_aug,28])\n        tras_28_y_n[pos_min] = tras_28_y_n[pos_min] * (1 - rando2[data_aug,28])  \n        tras_28_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_28_y_n),exp_std[1] * d[data_aug,28],exp_mean[1] + exp_std[1] * rando[data_aug,28] * d[data_aug,28]))      \n       \n        # 29 vertical traslation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_29_y_n = cs(x_n) + rando[data_aug,29] \n        tras_29_y_n[pos_ran] = tras_29_y_n[pos_ran] * (1 + rando[data_aug,29])\n        tras_29_y_n[pos_max] = tras_29_y_n[pos_max] * (1 + rando2[data_aug,29])\n        tras_29_y_n[pos_min] = tras_29_y_n[pos_min] * (1 - rando2[data_aug,29])\n        tras_29_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_29_y_n),exp_std[1],rando_de[29]))\n        \n        # 30 vertical dilatation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_30_y_n = cs(x_n) * d[data_aug,30]\n        tras_30_y_n[pos_ran] = tras_30_y_n[pos_ran] * (1 + rando[data_aug,30])\n        tras_30_y_n[pos_max] = tras_30_y_n[pos_max] * (1 + rando2[data_aug,30])\n        tras_30_y_n[pos_min] = tras_30_y_n[pos_min] * (1 - rando2[data_aug,30])\n        tras_30_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_30_y_n),exp_std[1] * d[data_aug,30],exp_mean[1]))\n        \n        # 31 vertical traslation + vertical dilatation + min and max as outliers + random point \n        pos_ran = random.randint(0, len(orig_s[0]) - 2)\n        tras_31_y_n = (cs(x_n) + rando[data_aug,31]) * d[data_aug,31]\n        tras_31_y_n[pos_ran] = tras_31_y_n[pos_ran] * (1 + rando[data_aug,31])\n        tras_31_y_n[pos_max] = tras_31_y_n[pos_max] * (1 + rando2[data_aug,31])\n        tras_31_y_n[pos_min] = tras_31_y_n[pos_min] * (1 - rando2[data_aug,31])  \n        tras_31_y_de_n = np.ndarray.tolist(deNorm(np.array(tras_31_y_n),exp_std[1] * d[data_aug,31],exp_mean[1] + exp_std[1] * rando[data_aug,31] * d[data_aug,31]))      \n        \n    \n        # compute the curvematching for the three transformations\n        a = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_y_de_n, numberOfBootstrapVariations=1)\n        b = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_2_y_de_n, numberOfBootstrapVariations=1)\n        c = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_3_y_de_n, numberOfBootstrapVariations=1)\n        e = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_4_y_de_n, numberOfBootstrapVariations=1)        \n        f = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_5_y_de_n, numberOfBootstrapVariations=1)\n        g = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_6_y_de_n, numberOfBootstrapVariations=1)\n        h = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_7_y_de_n, numberOfBootstrapVariations=1)\n        i = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_8_y_de_n, numberOfBootstrapVariations=1)\n        j = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_9_y_de_n, numberOfBootstrapVariations=1)        \n        k = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_10_y_de_n, numberOfBootstrapVariations=1)\n        l = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_11_y_de_n, numberOfBootstrapVariations=1)\n        m = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_12_y_de_n, numberOfBootstrapVariations=1)\n        n = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_13_y_de_n, numberOfBootstrapVariations=1)\n        o = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_14_y_de_n, numberOfBootstrapVariations=1)\n        p = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_15_y_de_n, numberOfBootstrapVariations=1)\n        \n        q = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_16_y_de_n, numberOfBootstrapVariations=1)\n        r = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_17_y_de_n, numberOfBootstrapVariations=1)\n        s = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_18_y_de_n, numberOfBootstrapVariations=1)\n        t = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_19_y_de_n, numberOfBootstrapVariations=1)        \n        u = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_20_y_de_n, numberOfBootstrapVariations=1)\n        v = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_21_y_de_n, numberOfBootstrapVariations=1)\n        w = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_22_y_de_n, numberOfBootstrapVariations=1)\n        x = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_23_y_de_n, numberOfBootstrapVariations=1)\n        y = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_24_y_de_n, numberOfBootstrapVariations=1)        \n        z = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_25_y_de_n, numberOfBootstrapVariations=1)\n        a2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_26_y_de_n, numberOfBootstrapVariations=1)\n        b2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_27_y_de_n, numberOfBootstrapVariations=1)\n        c2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_28_y_de_n, numberOfBootstrapVariations=1)\n        e2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_29_y_de_n, numberOfBootstrapVariations=1)\n        f2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_30_y_de_n, numberOfBootstrapVariations=1)\n        g2 = db.executeCurveMatching(x_sim=x_de_n, y_sim=y_de_n, x_exp=x_de_n, y_exp=tras_31_y_de_n, numberOfBootstrapVariations=1)\n                \n        \n        # the output of the curvematching is in json format\n        a_1 = json.loads(a)  # 1 trasl\n        b_1 = json.loads(b)  # 2 dil\n        c_1 = json.loads(c)  # 3 trasl + dil\n        e_1 = json.loads(e)  # 4 max as outlier\n        f_1 = json.loads(f)  # 5 trasl + max as outlier\n        g_1 = json.loads(g)  # 6 dil + max as outlier\n        h_1 = json.loads(h)  # 7 trasl + dil + max as outlier\n        i_1 = json.loads(i)  # 8 min as outlier\n        j_1 = json.loads(j)  # 9 trasl + min as outlier\n        k_1 = json.loads(k)  # 10 dil + min as outlier\n        l_1 = json.loads(l)  # 11 trasl + dil + min as outlier\n        m_1 = json.loads(m)  # 12 min and max as outliers\n        n_1 = json.loads(n)  # 13 trasl +  min and max as outliers\n        o_1 = json.loads(o)  # 14 dil + min and max as outliers\n        p_1 = json.loads(p)  # 15 trasl + dil + min and max as outliers\n        \n        q_1 = json.loads(q)  # 16 random point\n        r_1 = json.loads(r)  # 17 trasl + random point\n        s_1 = json.loads(s)  # 18 dil + random point\n        t_1 = json.loads(t)  # 19 max as outlier + random point\n        u_1 = json.loads(u)  # 20 min as outlier + random point\n        v_1 = json.loads(v)  # 21 max and min as outliers + random point\n        w_1 = json.loads(w)  # 22 trasl + dil + random point\n        x_1 = json.loads(x)  # 23 trasl + max as outlier + random point\n        y_1 = json.loads(y)  # 24 trasl + min as outlier + random point\n        z_1 = json.loads(z)  # 25 dil + max as outlier + random point\n        a2_1 = json.loads(a2)  # 26 dil + min as outlier + random point\n        b2_1 = json.loads(b2)  # 27 trasl + dil + max as outlier + random point\n        c2_1 = json.loads(c2)  # 28 trasl + dil + min as outlier + random point\n        e2_1 = json.loads(e2)  # 29 trasl + min and max as outliers + random point\n        f2_1 = json.loads(f2)  # 30 dil + min and max as outliers + random point\n        g2_1 = json.loads(g2)  # 31 trasl + dil + min and max as outliers + random point\n\n        \n        # the final format is a dataframe\n        df = df.append(pd.DataFrame([a_1]))\n        df = df.append(pd.DataFrame([b_1]))\n        df = df.append(pd.DataFrame([c_1]))\n        df = df.append(pd.DataFrame([e_1]))\n        df = df.append(pd.DataFrame([f_1]))\n        df = df.append(pd.DataFrame([g_1]))\n        df = df.append(pd.DataFrame([h_1]))\n        df = df.append(pd.DataFrame([i_1]))\n        df = df.append(pd.DataFrame([j_1]))\n        df = df.append(pd.DataFrame([k_1]))\n        df = df.append(pd.DataFrame([l_1]))\n        df = df.append(pd.DataFrame([m_1]))\n        df = df.append(pd.DataFrame([n_1]))\n        df = df.append(pd.DataFrame([o_1]))\n        df = df.append(pd.DataFrame([p_1]))\n        \n        df = df.append(pd.DataFrame([q_1]))\n        df = df.append(pd.DataFrame([r_1]))\n        df = df.append(pd.DataFrame([s_1]))\n        df = df.append(pd.DataFrame([t_1]))\n        df = df.append(pd.DataFrame([u_1]))\n        df = df.append(pd.DataFrame([v_1]))\n        df = df.append(pd.DataFrame([w_1]))\n        df = df.append(pd.DataFrame([x_1]))\n        df = df.append(pd.DataFrame([y_1]))\n        df = df.append(pd.DataFrame([z_1]))\n        df = df.append(pd.DataFrame([a2_1]))\n        df = df.append(pd.DataFrame([b2_1]))\n        df = df.append(pd.DataFrame([c2_1]))\n        df = df.append(pd.DataFrame([e2_1]))\n        df = df.append(pd.DataFrame([f2_1]))\n        df = df.append(pd.DataFrame([g2_1]))\n        \n        # 1 vertical traslation\n        ls_f1.append(rando_de[1])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 2 vertical dilatation\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,2])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 3 vertical traslation + vertical dilatation\n        ls_f1.append(rando_de[3] * d[data_aug,3])\n        ls_f2.append(d[data_aug,3])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 4 max as outlier\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,4])\n        ls_f4.append(0)\n        ls_f5.append(0)\n       \n        # 5 vertical traslation + max as outlier\n        ls_f1.append(rando_de[5])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,5])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 6 vertical dilatation + max as outlier\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,6])\n        ls_f3.append(rando2[data_aug,6])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 7 vertical traslation + vertical dilatation + max as outlier\n        ls_f1.append(rando_de[7] * d[data_aug,7])\n        ls_f2.append(d[data_aug,7])\n        ls_f3.append(rando2[data_aug,7])\n        ls_f4.append(0)\n        ls_f5.append(0)\n        \n        # 8 min as outlier\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,8])\n        ls_f5.append(0)\n        \n        # 9 vertical traslation + min as outlier\n        ls_f1.append(rando_de[9])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,9])\n        ls_f5.append(0)\n        \n        # 10 vertical dilatation + min as outlier\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,10])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,10])\n        ls_f5.append(0)\n        \n        # 11 vertical traslation + vertical dilatation + min as outlier\n        ls_f1.append(rando_de[11] * d[data_aug,11])\n        ls_f2.append(d[data_aug,11])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,11])\n        ls_f5.append(0)\n        \n        # 12 min and max as outliers\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,12])\n        ls_f4.append(-rando2[data_aug,12])\n        ls_f5.append(0)\n        \n        # 13 vertical traslation + min and max as outliers\n        ls_f1.append(rando_de[13])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,13])\n        ls_f4.append(-rando2[data_aug,13])\n        ls_f5.append(0)\n        \n        # 14 vertical dilatation + min and max as outliers\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,14])\n        ls_f3.append(rando2[data_aug,14])\n        ls_f4.append(-rando2[data_aug,14])\n        ls_f5.append(0)\n        \n        # 15 vertical traslation + vertical dilatation + min and max as outliers\n        ls_f1.append(rando_de[15] * d[data_aug,15])\n        ls_f2.append(d[data_aug,15])\n        ls_f3.append(rando2[data_aug,15])\n        ls_f4.append(-rando2[data_aug,15])\n        ls_f5.append(0)\n        \n        # 16 random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,16])\n        \n        # 17 vertical traslation + random point\n        ls_f1.append(rando_de[17])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,17])\n        \n        # 18 vertical dilatation + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,18])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,18])\n        \n        # 19 max as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,19])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,19])\n        \n        # 20 min as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,20])\n        ls_f5.append(rando[data_aug,20])\n        \n        # 21 max and min as outliers + random point\n        ls_f1.append(0)\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,21])\n        ls_f4.append(-rando2[data_aug,21])\n        ls_f5.append(rando[data_aug,21])\n        \n        # 22 vertical traslation + vertical dilatation + random point\n        ls_f1.append(rando_de[22] * d[data_aug,22])\n        ls_f2.append(d[data_aug,22])\n        ls_f3.append(0)\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,22])\n        \n        # 23 vertical traslation + max as outlier + random point\n        ls_f1.append(rando_de[23])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,23])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,23])\n        \n        # 24 vertical traslation + min as outlier + random point\n        ls_f1.append(rando_de[24])\n        ls_f2.append(0)\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,24])\n        ls_f5.append(rando[data_aug,24])\n        \n        # 25 vertical dilatation + max as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,25])\n        ls_f3.append(rando2[data_aug,25])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,25])\n        \n        # 26 vertical dilatation + min as outlier + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,26])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,26])\n        ls_f5.append(rando[data_aug,26])\n        \n        # 27 vertical traslation + vertical dilatation + max as outlier + random point\n        ls_f1.append(rando_de[27] * d[data_aug,27])\n        ls_f2.append(d[data_aug,27])\n        ls_f3.append(rando2[data_aug,27])\n        ls_f4.append(0)\n        ls_f5.append(rando[data_aug,27])\n        \n        # 28 vertical traslation + vertical dilatation + min as outlier + random point\n        ls_f1.append(rando_de[28] * d[data_aug,28])\n        ls_f2.append(d[data_aug,28])\n        ls_f3.append(0)\n        ls_f4.append(-rando2[data_aug,28])\n        ls_f5.append(rando[data_aug,28])\n        \n        # 29 vertical traslation + min and max as outliers + random point\n        ls_f1.append(rando_de[29])\n        ls_f2.append(0)\n        ls_f3.append(rando2[data_aug,29])\n        ls_f4.append(-rando2[data_aug,29])\n        ls_f5.append(rando[data_aug,29])\n        \n        # 30 vertical dilatation + min and max as outliers + random point\n        ls_f1.append(0)\n        ls_f2.append(d[data_aug,30])\n        ls_f3.append(rando2[data_aug,30])\n        ls_f4.append(-rando2[data_aug,30])\n        ls_f5.append(rando[data_aug,30])\n        \n        # 31 vertical traslation + vertical dilatation + min and max as outliers + random point\n        ls_f1.append(rando_de[31] * d[data_aug,31])\n        ls_f2.append(d[data_aug,31])\n        ls_f3.append(rando2[data_aug,31])\n        ls_f4.append(-rando2[data_aug,31])\n        ls_f5.append(rando[data_aug,31])\n        \n    # add the new features as columns    \n    features1 = \"vert_trans_f\"\n    features2 = \"dil_vert_f\"\n    features3 = \"outlier on the max\"\n    features4 = \"outlier on the min\"\n    features5 = \"modfication on a single random point\"\n    \n    df[features1] = ls_f1\n    df[features2] = ls_f2\n    df[features3] = ls_f3\n    df[features4] = ls_f4\n    df[features5] = ls_f5\n    \n    df = df.reset_index()\n    df = df.drop([\"error\",\"index\"],axis=1)\n\n    # if the curvematching yields a wrong result, drop it\n    ind = df[df[\"score\"] < 0].index\n    df = df.drop(ind,axis = 0)\n    \n    # reorder the indexes of the dataframe\n    df = df.reset_index()\n    df = df.drop([\"index\"],axis=1) \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T20:05:59.741246Z","iopub.execute_input":"2022-05-26T20:05:59.741504Z","iopub.status.idle":"2022-05-26T20:05:59.887602Z","shell.execute_reply.started":"2022-05-26T20:05:59.741475Z","shell.execute_reply":"2022-05-26T20:05:59.886591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to test the function\naaa2 = prova(1061,2)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:17.720014Z","iopub.execute_input":"2022-05-25T09:31:17.72026Z","iopub.status.idle":"2022-05-25T09:31:48.575564Z","shell.execute_reply.started":"2022-05-25T09:31:17.720233Z","shell.execute_reply":"2022-05-25T09:31:48.574837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# multiple estraction from \tlaminar burning velocity measurement\n# num_id = the number of experiments that we want to extract from the db \n# num_data_aug = it is the number of desired data augmentation (notice that is always mulplied by 3)\n# Notice that this function will call many times the function laminar_burning_velocity_ex and it will\n# store only the dataframe that follow the following rules:\n# - the experiment must have these two columns:  'equivalence ratio' and 'laminar burning velocity'\n# - result from the curve matching must be not negative\n# - the used units are 'unitless' and 'cm/s' respectively for the 'equivalence ratio' and 'laminar burning velocity'\n# All those db results that don't follow these rules,they will be dropped\ndef laminar_burning_velocity_multiple_ex(num_id,num_data_aug):\n    \n    ex = db.filterDatabase(model_name='Experiment', experiment_type = 'laminar burning velocity measurement')\n\n    ex_id = [ex[i].id for i in range(0,len(ex))]\n    count = 0\n    df_list = []\n    \n    for i in ex_id:\n        \n        print(i)\n        df = prova(i,num_data_aug)# laminar_burning_velocity_ex(i,num_data_aug)\n\n        # drop the empty/irregular dataframe\n        if df.empty:\n            print('DataFrame is empty!')\n        else:\n            count = count + 1\n            df_list.append(df)\n            \n        if count == num_id:\n            break\n       \n    # concatenate the dataframes\n    df_tot = pd.concat(df_list)\n    \n    # reset the indexes\n    df_tot = df_tot.reset_index()\n    df_tot = df_tot.drop([\"index\"],axis=1)\n    \n    # write a csv\n    df_tot.to_csv('laminar_burning_velocity.csv', index=False)\n    \n    # to read a csv\n    # df2 = pd.read_csv('laminar_burning_velocity.csv')\n    \n    return df_tot\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:02:32.945615Z","iopub.execute_input":"2022-05-25T14:02:32.945902Z","iopub.status.idle":"2022-05-25T14:02:32.954381Z","shell.execute_reply.started":"2022-05-25T14:02:32.945863Z","shell.execute_reply":"2022-05-25T14:02:32.953246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to test the fucntion\naaa = laminar_burning_velocity_multiple_ex(15,15)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:03:11.858782Z","iopub.execute_input":"2022-05-25T14:03:11.859391Z","iopub.status.idle":"2022-05-25T14:49:29.040313Z","shell.execute_reply.started":"2022-05-25T14:03:11.859346Z","shell.execute_reply":"2022-05-25T14:49:29.039319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# notice that the name of the dataset may be different and the path must be edit\nX = pd.read_csv('../input/laminar22500/parziale_22_5k.csv')\n\n# csv for multi-label classification\nX.loc[X['vert_trans_f'] != 0,'vert_trans_f'] = 1\nX.loc[X['dil_vert_f'] != 0,'dil_vert_f'] = 1\nX.loc[X['outlier on the max'] != 0,'outlier on the max'] = 1\nX.loc[X['outlier on the min'] != 0,'outlier on the min'] = 1\nX.loc[X['modfication on a single random point'] != 0,'modfication on a single random point'] = 1\n#X.to_csv('laminar4.csv', index=False)\n\n# csv for multi-class single-label classification\na = (X['dil_vert_f'] != 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nb = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] != 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nc = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] != 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nd = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] != 0) & (X['modfication on a single random point'] == 0)\ne = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] != 0)\nt = a | b | c | d | e\nX = X[t]\nX = X.reset_index()\nX = X.drop([\"index\"],axis=1)\n#X.to_csv('laminar_burning_velocity_for_binary_classification.csv', index=False)\n\n# csv for multiclass single-label classification with only one new feature: target\na = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] != 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nb = (X['dil_vert_f'] != 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nc = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] != 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] == 0)\nd = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] != 0) & (X['modfication on a single random point'] == 0)\ne = (X['dil_vert_f'] == 0) & (X['vert_trans_f'] == 0) & (X['outlier on the max'] == 0) & (X['outlier on the min'] == 0) & (X['modfication on a single random point'] != 0)\nX.loc[a,'target'] = 0  # vert_trans_f\nX.loc[b,'target'] = 1  # dil_vert_f\nX.loc[c,'target'] = 2  # outlier on the max\nX.loc[d,'target'] = 3  # outlier on the min\nX.loc[e,'target'] = 4  # modfication on a single random point\nX = X.reset_index()\nX = X.drop([\"index\",\"vert_trans_f\",\"dil_vert_f\",\"outlier on the max\",\"outlier on the min\",\"modfication on a single random point\"],axis=1)\n# X = X.drop([\"Unnamed: 0\"],axis=1)\nX.to_csv('laminar22500_c.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:47:19.667543Z","iopub.execute_input":"2022-05-27T12:47:19.667844Z","iopub.status.idle":"2022-05-27T12:47:20.084416Z","shell.execute_reply.started":"2022-05-27T12:47:19.667814Z","shell.execute_reply":"2022-05-27T12:47:20.08342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalization of all the columns except target\ndef normalizeArray(ar):\n    return (ar - np.amin(ar))/(np.amax(ar)- np.amin(ar))\n\n# notice that the name of the dataset may be different and the path must be edit\ndf = pd.read_csv('./laminar22500_c.csv')\nprint(df)\n\nname = df.columns.to_list()\nfor i in name:\n    if(i == 'target'):\n        continue\n    df[i] = normalizeArray(np.array(df[i].to_list()))\nprint(df)\ndf.to_csv('lami.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#profiling of the dataset\npip install pandas-profiling","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\n# notice that the name of the dataset may be different and the path must be edit\ndf = pd.read_csv('../input/laminar4750-c-1/laminar4750_c (2).csv')\nprofile1 = ProfileReport(df, title=\"Pandas Profiling Report\")\nprofile1\n# notice that correlation function is studied from here","metadata":{},"execution_count":null,"outputs":[]}]}