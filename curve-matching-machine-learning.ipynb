{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nimport os\nimport tensorflow as tf\nimport keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T12:53:40.427942Z","iopub.execute_input":"2022-05-29T12:53:40.428757Z","iopub.status.idle":"2022-05-29T12:53:40.440131Z","shell.execute_reply.started":"2022-05-29T12:53:40.428708Z","shell.execute_reply":"2022-05-29T12:53:40.439357Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# X will be our dataset\n# it must be edit with the right path and dataset name\nX = pd.read_csv('../input/laminar22500-c-n/lami.csv')\n\ny = X[['target']]\nX = X.drop(['target'], axis= 1)\n\nX = np.array(X)\ny = y.iloc[:,0].values\n\n# dummy_y is the one-hot encoding of the categorical variable y\ndummy_y = np_utils.to_categorical(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:53:41.962834Z","iopub.execute_input":"2022-05-29T12:53:41.963472Z","iopub.status.idle":"2022-05-29T12:53:42.040558Z","shell.execute_reply.started":"2022-05-29T12:53:41.963437Z","shell.execute_reply":"2022-05-29T12:53:42.039739Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the units of the first layer with an architecture with only one hidden layer\n\n# define baseline model with only one hidden layer. This reflects The Universal Approximation Theorem,\n# which states that a neural network with 1 hidden layer can approximate any continuous function \n# for inputs within a specific range. So, this is the starting point for the model. The last\n# dense will have a number of units equal to the number of classes, while the only hidden layer\n# will have a number of units to be decided with crossvalidation.\n# In this cell and in the following one we will always use a k-fold crossvalidation with K = 5 and\n# we will always value the validation accuracy. \n# Notice that we computed for each model the mean among the results of the K different splits.\n# Notice also that we will use the ealy stopping to stop the training when the validation error \n# increases for a certain number (patience) of iterations. Finally, it restores the weigths of\n# the best iteration.\n\n# results:\n# units  val_accuracy\n# 100    0.6348888874053955\n# 500    0.6449333310127259\n# 1000   0.6485777735710144\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nunits = [100,500,1000]\n\nfor i in units:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(units)):\n    print(units[i], \" \", lista[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:01:51.677639Z","iopub.execute_input":"2022-05-28T08:01:51.678046Z","iopub.status.idle":"2022-05-28T08:32:00.911503Z","shell.execute_reply.started":"2022-05-28T08:01:51.678014Z","shell.execute_reply":"2022-05-28T08:32:00.91074Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the units of the first and second layers with an architecture with two hidden layers\n\n# results:\n# units  val_accuracy\n# 100    0.6789333343505859\n# 500    0.6940444350242615 \n# 1000   0.6965333342552185\n# 5000   0.6961777687072754\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nunits = [100,500,1000]\n\nfor i in units:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(units)):\n    print(units[i], \" \", lista[i])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T19:44:54.390365Z","iopub.execute_input":"2022-05-27T19:44:54.390781Z","iopub.status.idle":"2022-05-27T19:45:59.208414Z","shell.execute_reply.started":"2022-05-27T19:44:54.39075Z","shell.execute_reply":"2022-05-27T19:45:59.206865Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the probability of the dropout with an architecture with two hidden dense layers\n\n# here we are trying to evaluate a model with 2 hidden layers and a dropout before the output layer. \n# we can notice from the results that the dropout doesn't help\n\n# results:\n# prob  val_accuracy\n# 0.0   0.6957777857780456\n# 0.25   0.6911110997200012\n# 0.5   0.6764444351196289\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dropout(x),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nprob = np.linspace(0,1,5).tolist()[0:3]\n\nfor i in prob:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(500,i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(prob)):\n    print(prob[i], \" \", lista[i])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T20:24:00.698232Z","iopub.execute_input":"2022-05-27T20:24:00.698698Z","iopub.status.idle":"2022-05-27T21:09:01.323289Z","shell.execute_reply.started":"2022-05-27T20:24:00.698667Z","shell.execute_reply":"2022-05-27T21:09:01.322171Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the probability of the dropout with an architecture with two hidden dense layers\n\n# here we are trying to evaluate a model with 2 hidden layers and a dropout between the two hidden layers.\n# we can notice from the results that the dropout doesn't help\n\n# results:\n# prob  val_accuracy\n# 0.0   0.6952444434165954\n# 0.25   0.6835999965667725\n# 0.5   0.6672000050544739\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dropout(x),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nprob = np.linspace(0,1,5).tolist()[0:3]\n\nfor i in prob:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(500,i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(prob)):\n    print(prob[i], \" \", lista[i])\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:20:22.569558Z","iopub.execute_input":"2022-05-27T21:20:22.569982Z","iopub.status.idle":"2022-05-27T22:05:05.669356Z","shell.execute_reply.started":"2022-05-27T21:20:22.569949Z","shell.execute_reply":"2022-05-27T22:05:05.668218Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the units of the three hidden dense layers\n\n# we can see that it's performing better than the previous architectures\n\n# results:\n# units  val_accuracy\n# 100   0.7027111053466797\n# 500   0.7086666703224183\n# 1000   0.7111555576324463\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nunits = [100,500,1000]\n\nfor i in units:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(units)):\n    print(units[i], \" \", lista[i])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:15:55.745048Z","iopub.execute_input":"2022-05-28T09:15:55.745476Z","iopub.status.idle":"2022-05-28T09:46:25.638071Z","shell.execute_reply.started":"2022-05-28T09:15:55.745443Z","shell.execute_reply":"2022-05-28T09:46:25.637267Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the units of the four hidden dense layers\n\n# we can see that it's performing better than the previous architectures and units = 500 has\n# the best performance in val_accuracy\n\n# results:\n# units  val_accuracy\n# 100   0.7044444441795349\n# 500   0.7208444476127625\n# 1000   0.7123555541038513\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nunits = [100,500,1000]\n\nfor i in units:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(units)):\n    print(units[i], \" \", lista[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:48:43.73657Z","iopub.execute_input":"2022-05-28T09:48:43.737365Z","iopub.status.idle":"2022-05-28T10:20:50.784777Z","shell.execute_reply.started":"2022-05-28T09:48:43.737333Z","shell.execute_reply":"2022-05-28T10:20:50.783996Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the units of the five hidden dense layers\n\n# Notice that in this case the results are worse than those of the architecture with 4 hidden dense layers\n# So, we can see that we can stop around four or five hidden dense layers. In the next cell we will try to\n# see if it possibile to increase the val_accuracy on the 5-dense-layers architecture with some dropout layer.\n\n# results:\n# units  val_accuracy\n# 100   0.7143555521965027\n# 500   0.7064888834953308\n# 1000   0.7032888889312744\n\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nunits = [100,500,1000]\n\nfor i in units:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(units)):\n    print(units[i], \" \", lista[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T10:22:37.898076Z","iopub.execute_input":"2022-05-28T10:22:37.898426Z","iopub.status.idle":"2022-05-28T10:56:02.082629Z","shell.execute_reply.started":"2022-05-28T10:22:37.898397Z","shell.execute_reply":"2022-05-28T10:56:02.081797Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the probability of the dropout layer in an architecture with 5 hidden dense layers\n\n# Notice that in this case we will use units = 100, because it showed the best results in the previous crossvalidation\n# on the architecture with 5 hidden dense layers. But it doesn't help.\n\n# results:\n# prob  val_accuracy\n# 0.2   0.7085333347320557\n# 0.4   0.6967111110687256\n# 0.6   0.6939555525779724\n\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dropout(x),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nprob = [0.2,0.4,0.6]\n\nfor i in prob:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(100,i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(prob)):\n    print(prob[i], \" \", lista[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T11:45:34.600168Z","iopub.execute_input":"2022-05-28T11:45:34.600555Z","iopub.status.idle":"2022-05-28T12:19:04.899477Z","shell.execute_reply.started":"2022-05-28T11:45:34.600523Z","shell.execute_reply":"2022-05-28T12:19:04.898661Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the probability of the dropout layer in an architecture with 4 hidden dense layers\n\n# Notice that in this case we will use units = 500, because it showed the best results in the previous crossvalidation\n# on the architecture with 4 hidden dense layers. But it doesn't help. So, we will not use any dropout layers. \n\n# results:\n# prob  val_accuracy\n# 0.2   0.7083555698394776\n# 0.4   0.7010222196578979\n# 0.6   0.6789777755737305\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dropout(x),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nprob = [0.2,0.4,0.6]\n\nfor i in prob:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(500,i)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(prob)):\n    print(prob[i], \" \", lista[i])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T14:20:24.188368Z","iopub.execute_input":"2022-05-28T14:20:24.188872Z","iopub.status.idle":"2022-05-28T14:53:47.024392Z","shell.execute_reply.started":"2022-05-28T14:20:24.188837Z","shell.execute_reply":"2022-05-28T14:53:47.02352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the batch_size in an architecture with 4 hidden dense layers with 500 units each\n\n# In this case we want to choose the batch_size, starting from the stochastic gradient descent (batch_size = 1) \n# to the mini-batch gradeint descent (batch_size = 40). Given this large dataset, the batch size equal to ten seems a good trade-off\n# between training speed and val_accuracy.\n\n# results:\n# batch_size  val_accuracy\n# 1           0.644266664981842\n# 5           0.645377779006958\n# 10          0.6481777906417847\n# 20          0.6420444369316101\n# 40          0.6357777714729309\n\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nbatch_size = [1,5,10,20,40]\n\nfor i in batch_size:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        model = baseline_model(500)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=i, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(batch_size)):\n    print(batch_size[i], \" \", lista[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T07:27:13.490885Z","iopub.execute_input":"2022-05-28T07:27:13.491297Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the learning rate \n\n# Up to now the best model has: batch size = 10, 4 layers each of 500 units and no dropout layers\n# Now, we are going to tune one of the most important parameter: the learning rate. The deafault\n# learning rate is 0.001, but we are going to realize a grid with learning rate that goes from\n# 10^-1 to 10^-5. Notice that the optimizer used is Adam.\n# In this case the best learning rate is 0.0001.\n\n# results:\n# learning_rate  val_accuracy\n# 0.1            0.20764444172382354\n# 0.01           0.6019999980926514\n# 0.001          0.7085777640342712\n# 0.0001         0.7168444395065308\n# 1e-05          0.6318666696548462\n# 1e-06          0.5545777916908264\n\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer=x, metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nlr = [10**float(i) for i in np.arange(-1,-7,-1)]\n\nfor i in lr:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        opt = tf.keras.optimizers.Adam(learning_rate=i)\n        model = baseline_model(500,opt)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(lr)):\n    print(lr[i], \" \", lista[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T16:38:58.222801Z","iopub.execute_input":"2022-05-28T16:38:58.223333Z","iopub.status.idle":"2022-05-28T16:54:57.220559Z","shell.execute_reply.started":"2022-05-28T16:38:58.223275Z","shell.execute_reply":"2022-05-28T16:54:57.219444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crossvalidation on the learning rate around the previous best result\n\n# In this case the best learning rate is 0.0005.\n\n# results:\n# learning_rate  val_accuracy\n# 0.0005   0.7325333237648011\n# 5e-05    0.6845333337783813\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer=x, metrics=['accuracy'])\n    return model\n\nnum_epochs = 25\nkf = KFold(n_splits = 5, shuffle=True)\nn = X.shape[0]\n\nHistoriesOfStories = []\n\ncallbacks_list = [es]\nlr = [0.0005,0.00005 ]\n\nfor i in lr:\n    print(\"evaluating: \",i)\n    count = 0\n    Histories = []\n    for train_index, val_index in kf.split(np.zeros(n),y):\n        \n        print(\"count: \",count)\n        \n        X_train = X[train_index,:]\n        X_test = X[val_index,:]\n\n        y_train = dummy_y[train_index,:]\n        y_test  = dummy_y[val_index,:]\n\n        opt = tf.keras.optimizers.Adam(learning_rate=i)\n        model = baseline_model(500,opt)\n\n        # FIT THE MODEL\n        history = model.fit(X_train,\n                            y_train,\n                            epochs=num_epochs,\n                            callbacks=callbacks_list,\n                            validation_data=(X_test,y_test),\n                            batch_size=10, \n                            verbose=1)\n\n\n        # LOAD BEST MODEL to evaluate the performance of the model\n        #model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n        Histories.append(history)\n\n        tf.keras.backend.clear_session()\n        count = count + 1\n        \n    HistoriesOfStories.append(Histories)\n    \nlista = []\nfor ele in HistoriesOfStories:\n    lista2 = []\n    for ala in ele:\n        lista2.append(np.max(ala.history[\"val_accuracy\"]))\n    lista.append(np.mean(lista2))\nfor i in np.arange(0,len(lr)):\n    print(lr[i], \" \", lista[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T17:20:22.591273Z","iopub.execute_input":"2022-05-28T17:20:22.592019Z","iopub.status.idle":"2022-05-28T17:49:07.950189Z","shell.execute_reply.started":"2022-05-28T17:20:22.591989Z","shell.execute_reply":"2022-05-28T17:49:07.949067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this cell we are evaluating the model with the best parameters chosen in the previous cells. \n# To recap: 4 hidden dense layers with 500 units each, no dropout layers, batch size equal to 10 and learning rate equal to 0.0005.\n# We will choose the final model trough a k-fold crossvalidation with K = 3. The model with the highest val_accuracy will be \n# selected as classifier.\n\nes = EarlyStopping(monitor='val_accuracy', \n                           mode='max',\n                           patience=8, \n                           restore_best_weights=True) \n\ndef baseline_model(y,x):\n    # create model\n    model = Sequential([\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(y, activation=\"relu\"),\n        Dense(5, activation='softmax'),\n    ])\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer=x, metrics=['accuracy'])\n    return model\n\nnum_epochs = 400\nkf = KFold(n_splits = 3, shuffle=True)\nn = X.shape[0]\n\ncallbacks_list = [es]\n\ncount = 0\nHistories = []\nmodels = []\n\nfor train_index, val_index in kf.split(np.zeros(n),y):\n\n    print(\"count: \",count)\n\n    X_train = X[train_index,:]\n    X_test = X[val_index,:]\n\n    y_train = dummy_y[train_index,:]\n    y_test  = dummy_y[val_index,:]\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n    model = baseline_model(500,opt)\n\n    # FIT THE MODEL\n    history = model.fit(X_train,\n                        y_train,\n                        epochs=num_epochs,\n                        callbacks=callbacks_list,\n                        validation_data=(X_test,y_test),\n                        batch_size=10, \n                        verbose=1)\n\n    models.append(model)\n\n    Histories.append(history)\n\n    tf.keras.backend.clear_session()\n    count = count + 1\n\n\n    \nlista = []\nfor ele in Histories:\n    lista.append(np.max(ele.history[\"val_accuracy\"]))\nfor i in np.arange(0,len(lista)):\n    print(lista[i])\n\nmodel_max = np.argmax(lista)\nfinal_model = models[model_max]    \nmodel.save('./model','.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:53:52.383536Z","iopub.execute_input":"2022-05-29T12:53:52.384659Z","iopub.status.idle":"2022-05-29T13:06:14.534453Z","shell.execute_reply.started":"2022-05-29T12:53:52.384610Z","shell.execute_reply":"2022-05-29T13:06:14.533460Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from numpy import loadtxt\nfrom keras.models import load_model\n \n# load model\nmodel = load_model('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:17:02.335772Z","iopub.execute_input":"2022-05-29T13:17:02.336134Z","iopub.status.idle":"2022-05-29T13:17:02.433655Z","shell.execute_reply.started":"2022-05-29T13:17:02.336105Z","shell.execute_reply":"2022-05-29T13:17:02.432829Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# evaluation of the model\n# model2.evaluate(X, dummy_y)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:17:36.279940Z","iopub.execute_input":"2022-05-29T13:17:36.280290Z","iopub.status.idle":"2022-05-29T13:17:38.397379Z","shell.execute_reply.started":"2022-05-29T13:17:36.280262Z","shell.execute_reply":"2022-05-29T13:17:38.396549Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# the following cells were used to generate the plots for the presentations and the values are those saved in the previous cells\n\nimport numpy as np\nimport matplotlib.pyplot as plt\na1 = np.array([0.6348888874053955, 0.6449333310127259, 0.6485777735710144]) #1 hidden layer\na2 = np.array([0.6789333343505859, 0.6940444350242615, 0.6965333342552185]) #2 hidden layer\na3 = np.array([0.7027111053466797, 0.7086666703224183, 0.7111555576324463]) #3 hidden layer\na4 = np.array([0.7044444441795349, 0.7208444476127625, 0.7123555541038513]) #4 hidden layer\na5 = np.array([0.7143555521965027, 0.7064888834953308, 0.7032888889312744]) #5 hidden layer\nx1 = np.array([100, 500, 1000]) #num of units\nplt.plot(x1, a1, marker = 'o', label='1 hidden layer')\nplt.plot(x1, a2, marker = 'o',label='2 hidden layer')\nplt.plot(x1, a3, marker = 'o',label='3 hidden layer')\nplt.plot(x1, a4, marker = 'o',label='4 hidden layer')\nplt.plot(x1, a5, marker = 'o',label='5 hidden layer')\nplt.xlabel('number of units')\nplt.ylabel('val_accuracy')\nplt.legend()\nplt.savefig('books_read1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T14:36:00.440707Z","iopub.execute_input":"2022-05-29T14:36:00.441113Z","iopub.status.idle":"2022-05-29T14:36:00.733164Z","shell.execute_reply.started":"2022-05-29T14:36:00.441071Z","shell.execute_reply":"2022-05-29T14:36:00.732422Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nx = [1e-06, 1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.01]\ny = [0.5545777916908264, 0.6318666696548462, 0.6845333337783813, 0.7168444395065308, 0.7325333237648011, 0.7085777640342712, 0.6019999980926514]\nplt.plot(x, y,  marker = 'o')\nplt.title('How val_accuracy changes with learning rate')\nplt.xlabel('learning rate')\nplt.ylabel('val_accuracy')\nplt.savefig('books_read.png')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(8, 6))\n#%matplotlib inline\n\nx1 = [0, 0.25, 0.5]\nx2 = [0, 0.2, 0.4, 0.6]\n\ny1 = [0.6957777857780456, 0.6911110997200012, 0.6764444351196289] # 2 hidden layer w/ dropout before output\ny2 = [0.6952444434165954, 0.6835999965667725, 0.6672000050544739] # 2 hidden layer w/ dropout in between the 2 hidden layers\n\ny3 = [0.7208444476127625, 0.7083555698394776, 0.7010222196578979, 0.6789777755737305] # 4 hidden layer w/ dropout\ny4 = [0.7143555521965027, 0.7085333347320557, 0.6967111110687256, 0.6939555525779724] # 5 hidden layers w/ dropout \n\n\n\nplt.plot(x1, y1,  marker = 'o', label='2 hidden layers w/ dropout before output')\nplt.plot(x1, y2,  marker = 'o', label='2 hidden layers w/ dropout in between the 2 hidden layers')\nplt.plot(x2, y3,  marker = 'o', label='4 hidden layers w/ dropout')\nplt.plot(x2, y4,  marker = 'o', label='5 hidden layers w/ dropout')\nplt.title('How val_accuracy changes with dropout')\nplt.xlabel('probability')\nplt.ylabel('val_accuracy')\nplt.legend(loc=3)\nplt.savefig('books_read.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:54:11.800861Z","iopub.execute_input":"2022-05-29T13:54:11.801682Z","iopub.status.idle":"2022-05-29T13:54:12.121552Z","shell.execute_reply.started":"2022-05-29T13:54:11.801640Z","shell.execute_reply":"2022-05-29T13:54:12.120766Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.arange(0, 69, 1)\n\ny1 = Histories[1].history['val_accuracy']\ny2 = Histories[1].history['accuracy']\n\nplt.plot(x, y1,  marker = '.', label=\"test accuracy\")\nplt.plot(x, y2,  marker = '.', label=\"training accuracy\")\n\nplt.title('plot for the final model: training and test accuracy')\nplt.xlabel('epoch_num')\nplt.ylabel('accuracy')\nplt.legend(loc=4)\nplt.savefig('books_read.png')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:43:13.088835Z","iopub.execute_input":"2022-05-29T13:43:13.089248Z","iopub.status.idle":"2022-05-29T13:43:13.357194Z","shell.execute_reply.started":"2022-05-29T13:43:13.089217Z","shell.execute_reply":"2022-05-29T13:43:13.356435Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.arange(0, 69, 1)\n\ny1 = Histories[1].history['loss']\ny2 = Histories[1].history['val_loss']\n\nplt.plot(x, y1,  marker = '.', label=\"training loss\")\nplt.plot(x, y2,  marker = '.', label=\"test loss\")\n\nplt.title('plot for the final model: training and test loss')\nplt.xlabel('epoch_num')\nplt.ylabel('loss')\nplt.legend(loc=3)\nplt.savefig('books_read2.png')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:44:20.687476Z","iopub.execute_input":"2022-05-29T13:44:20.688293Z","iopub.status.idle":"2022-05-29T13:44:20.973405Z","shell.execute_reply.started":"2022-05-29T13:44:20.688254Z","shell.execute_reply":"2022-05-29T13:44:20.972472Z"},"trusted":true},"execution_count":35,"outputs":[]}]}